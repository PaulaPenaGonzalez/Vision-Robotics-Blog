<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Localización del robot con AprilTags</title>
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <h1> Localización del robot utilizando AprilTags </h1>
        <p>Estimación de la pose del robot a partir de visión artificial y odometría.</p>
    </header>

    <!-- Introducción antes de las publicaciones -->
    <div class="container">
        <p>
        En este proyecto se ha implementado un sistema de localización de un robot móvil mediante el uso de balizas AprilTags y técnicas de visión por computador.
        </p>
        <p>
        A partir de las imágenes capturadas por la cámara del robot, se detectan las balizas y se calcula la posición y orientación del robot con respecto al mundo aplicando transformaciones geométricas (roto-traslaciones) entre diferentes sistemas de referencia: cámara, baliza, robot y mundo.
        </p>
        <p>
        Cuando el robot no es capaz de ver ninguna baliza, el sistema recurre a la odometría para estimar su nueva posición.
        </p>

        <!-- Imagen destacada del proyecto -->
        <div style="text-align: center; margin-top: 20px;">
            <img src="https://raw.githubusercontent.com/PaulaPenaGonzalez/Vision-Robotics-Blog/main/assets/images/proyecto3.png" 
                 alt="Localización del robot con AprilTags" 
                 style="width: 600px; height: auto; border-radius: 8px;">
        </div>
    </div>


    <div class="container">
        <h2>Publicaciones - Reconstrucción 3D</h2>
        <button onclick="mostrarPost('Dia1', 1)"> Primer post </button>
        <button onclick="mostrarPost('Dia2', 2)"> Segundo post</button>
        <button onclick="mostrarPost('Dia3', 3)"> Tercer post</button>
        <button onclick="mostrarPost('Dia4', 4)"> Cuarto post</button>
	<button onclick="mostrarPost('Dia5', 5)"> Quinto post</button>
	<button onclick="mostrarPost('Dia6', 6)"> Sexto post</button>
	<button onclick="mostrarPost('Dia7', 7)"> Séptimo post</button>
    </div>



    <!-- Contenido del primer post -->
    <div id="post_Dia1_1" class="post-container" style="display: none;">
	    <h2>Objetivo del proyecto y detección de balizas AprilTags</h2>
	    
	    <h3> Objetivo principal</h3>
	    <p>
	        El objetivo de este proyecto es implementar un sistema de localización autónoma de un robot móvil en un entorno cerrado utilizando balizas AprilTags como referencias visuales. 
	    </p>
	    <p>
	        Para lograrlo, se emplea una cámara montada sobre el robot y se calcula su posición y orientación (pose) a partir de la detección y análisis de las balizas. 
	        Cuando las balizas no están visibles, se recurre a los datos de odometría para mantener una estimación continua.
	    </p>
	    
	    <h3> Detección de AprilTags</h3>
	    <p>
	        Se utiliza la biblioteca <code>pyapriltags</code> para detectar las balizas en las imágenes capturadas por la cámara del robot. Esta herramienta proporciona las coordenadas de las esquinas de cada baliza y su identificador único.
	    </p>
	    <p>
	        A partir de estas esquinas, se aplica el algoritmo <code>solvePnP</code> de OpenCV para estimar la pose relativa de la cámara con respecto a la baliza. Esta información, combinada con la pose conocida de la baliza en el mundo, permite calcular la pose del robot mediante una serie de roto-traslaciones.
	    </p>
	    
	    <div style="text-align: center; margin-top: 20px;">
	        <img src="https://raw.githubusercontent.com/PaulaPenaGonzalez/Vision-Robotics-Blog/main/assets/images/apriltag.png" 
	             alt="Detección de AprilTags" 
	             style="width: 500px; height: auto; border-radius: 8px;">
	    </div>
	
	    <h3> Elección de la baliza más confiable</h3>
	    <p>
	        En cada imagen se pueden detectar múltiples balizas, pero no todas ofrecen la misma fiabilidad. 
	        Para seleccionar la mejor, se calcula el área proyectada de cada baliza en la imagen, y se elige aquella con mayor área aparente, ya que es la más cercana y ofrece menor error en la estimación de la pose.
	    </p>
	    <p>
	        Este criterio permite mejorar la precisión del sistema y evitar errores derivados de detecciones lejanas o parciales.
	    </p>
    </div>

    <!-- Contenido del segundo post -->
    <div id="post_Dia2_2" class="post-container" style="display: none;">
	    <h2>Estimación de la pose del robot mediante transformaciones</h2>
	    
	    <h3> Transformaciones homogéneas</h3>
	    <p>
	        A partir de la detección de una baliza AprilTag, se puede calcular la posición y orientación del robot respecto al mundo utilizando una serie de matrices de transformación homogéneas 4x4. 
	        El procedimiento se basa en transformar desde el sistema de referencia de la baliza hasta el del robot, pasando por la cámara.
	    </p>
	
	    <p>
	        La secuencia de transformaciones aplicada es la siguiente:
	    </p>
	
	    <pre>
	    T_robot_to_world = T_tag_to_world · T_rot · T_cam_to_tag · T_rot_inv · T_robot_to_cam
	    </pre>
	
	    <ul>
	        <li><strong>T_tag_to_world:</strong> Transformación conocida desde el archivo YAML que define la posición de la baliza en el mundo.</li>
	        <li><strong>T_cam_to_tag:</strong> Se calcula <code>T_tag_to_cam</code> con <code>solvePnP</code> a partir de las esquinas detectadas y realizando la inversa obtenemos <code>T_cam_to_tag</code>.</li>
	        <li><strong>T_robot_to_cam:</strong> En el archivo .SDF, define la posición relativa de la cámara respecto al centro del robot. Se invierte para obtener el robot con respecto a la cámara. </li>
		<li><strong>T_rot:</strong> Define la rotación de los ejes para pasar del sistema de referencia del simulador al marco óptico (OpenCV). </li>
	    </ul>
	
	    <h3> Extracción de la pose del robot</h3>
	    <p>
	        Una vez calculada la matriz <code>T_robot_to_world</code>, se extrae la posición del robot de la última columna de la matriz (x, y), y su orientación (yaw) a partir de los elementos de rotación con la fórmula:
	    </p>
	
	    <pre>
	    pitch = np.arctan2(-R_f[2, 0], np.sqrt(R_f[0, 0]**2 + R_f[1, 0]**2))
            yaw = np.arctan2(R_f[1, 0] / np.cos(pitch), R_f[0, 0] / np.cos(pitch))
	    </pre>
	
	    <p>
	        Esto nos da la orientación del robot en radianes respecto al eje Z del mundo, completando así su pose: (x, y, yaw).
	    </p>
	
	    <div style="text-align: center; margin-top: 20px;">
	        <img src="https://raw.githubusercontent.com/PaulaPenaGonzalez/Vision-Robotics-Blog/main/assets/images/localizacion.png" 
	             alt="Transformaciones para obtener la pose del robot" 
	             style="width: 520px; height: auto; border-radius: 8px;">
	    </div>
	
	    <h3> Consideración del sistema de referencia óptico</h3>
	    <p>
	        Como <code>solvePnP</code> devuelve la pose en el sistema de referencia óptico de la cámara (donde X va a la derecha, Y hacia abajo y Z hacia delante), se aplica una conversión a sistema estándar para que las transformaciones sean coherentes con la referencia del mundo y el robot. Esto se realiza mediante una rotación compuesta:
	    </p>
	
	    <pre>
	    R_optical_to_standard = RotX(-90°) · RotZ(-90°)
	    T_rot = np.eye(4)
            T_rot[:3, :3] = R_world_to_optical
	    </pre>
	
	    <p>
	        Esta rotación se incorpora directamente y, de la misma manera, también de forma inversa, en la multiplicación final: ida y vuelta a coordenadas del marco óptico (OpenCV).
	    </p>
    </div>


    <!-- Contenido del tercer post -->
    <div id="post_Dia3_3" class="post-container" style="display: none;">
	    <h2>Estimación de la pose con odometría</h2>
	    
	    <h3> ¿Qué ocurre cuando no se detectan AprilTags?</h3>
	    <p>
	        En muchas ocasiones, el robot puede perder de vista las balizas. Para evitar quedarse sin estimación de la pose, se emplea la <strong>odometría del robot</strong> como sistema de respaldo.
	    </p>
	    <p>
	        Dado que la odometría está sujeta a error acumulativo, no se usa como fuente primaria, sino que se integra solamente en los pasos en los que no se visualizan balizas. En cada iteración, se calcula el cambio de posición y orientación (delta) desde la última lectura, y se aplica sobre la última pose estimada por visión.
	    </p>
	
	    <p>
	        Se probó una alternativa basada en integrar velocidades lineales y angulares directamente:
	    </p>
	
	    <pre>
	    x += v * cos(theta) * dt
	    y += v * sin(theta) * dt
	    theta += w * dt
	    </pre>
	
	    <p>
	        Sin embargo, en la práctica, se observó que aprovechar directamente la odometría ofrecía mejores resultados, ya que el simulador ya entrega valores acumulados de posición y orientación.
	    </p>
	
	    <h3> Combinación de fuentes</h3>
	    <p>
	        El sistema implementado combina ambas técnicas:
	    </p>
	    <ul>
	        <li>Cuando se ve una baliza → se usa la pose estimada por visión y se actualiza el estado.</li>
	        <li>Cuando no se ve ninguna baliza → se actualiza la pose anterior con la diferencia de odometría.</li>
	    </ul>
	
	    <p>
	        Esta estrategia híbrida permite mantener una estimación continua y bastante estable de la posición del robot en el entorno simulado.
	    </p>
    </div>

    <!-- Contenido del cuarto post -->
    <div id="post_Dia4_4" class="post-container" style="display: none;">
	    <h2>Selección de la mejor baliza y visualización con colores</h2>
	
	    <h3> ¿Por qué seleccionar una sola baliza?</h3>
	    <p>
	        Cuando varias AprilTags son visibles en la imagen, la estimación de la pose puede variar entre unas y otras debido a la perspectiva o la distancia. Por ello, se decidió elegir únicamente la más <strong>confiable</strong> en cada iteración, para mantener la consistencia.
	    </p>
	    <p>
	        La selección se basa en el <strong>área del contorno</strong> detectado: cuanto mayor es, más cercana está la baliza y, por tanto, más precisa será su estimación.
	    </p>
	
	    <h3> Visualización mejorada</h3>
	    <p>
	        Para ayudar en el desarrollo y depuración del sistema, se implementó una función para pintar las balizas detectadas en la imagen:
	    </p>
	    <ul>
	        <li>Todas las balizas detectadas se pintan de <strong>verde</strong>.</li>
	        <li>La baliza seleccionada como la mejor se pinta de <strong>azul</strong>.</li>
	    </ul>
	
	    <p>
	        Esto permitió verificar visualmente qué baliza estaba siendo usada para estimar la pose, y ayudó a ajustar correctamente el criterio de selección.
	    </p>
	
	    <div style="text-align: center; margin-top: 20px;">
	        <img src="https://raw.githubusercontent.com/PaulaPenaGonzalez/Vision-Robotics-Blog/main/assets/images/balizas.png" 
	             alt="Selección de la mejor baliza con resaltado azul" 
	             style="width: 500px; height: auto; border-radius: 8px;">
	    </div>
	
	    <p>
	        Esta estrategia garantiza que siempre se utilice la baliza más cercana (y por tanto más precisa), al tiempo que se ofrece una visualización clara del sistema en acción.
	    </p>
    </div>


    <!-- Contenido del quinto post -->
    <div id="post_Dia5_5" class="post-container" style="display: none;">
	    <h2> Movimiento del Robot y Estrategia de Exploración </h2>
	
	    <p>
	        En esta etapa del proyecto, se integró el movimiento autónomo del robot para explorar el entorno en busca de balizas AprilTag. Esto fue fundamental para poder estimar su posición de forma continua, incluso cuando no se detectaban balizas durante algunos instantes.
	    </p>
	
	    <p>
	        El movimiento se controló utilizando los comandos <code>HAL.setV()</code> para la velocidad lineal y <code>HAL.setW()</code> para la velocidad angular. De esta forma, el robot pudo avanzar en línea recta mientras realizaba un giro constante, permitiéndole recorrer el espacio de forma helicoidal.
	    </p>
	
	    <p>
	        Además, para evitar que el robot quedase atascado o siguiera un patrón demasiado repetitivo, se incorporó un pequeño giro aleatorio usando la librería <code>random</code>. Esto hacía que el robot variara ligeramente su dirección de forma impredecible, maximizando así la probabilidad de detectar nuevas balizas.
	    </p>
	
	    <p>
	        Este comportamiento activo es especialmente útil en entornos cerrados o con obstáculos, donde las balizas pueden estar parcialmente ocultas o fuera del campo de visión en momentos puntuales.
	    </p>
	
    </div>

    <!-- Contenido del sexto post -->
    <div id="post_Dia6_6" class="post-container" style="display: none;">
	
	    <h2>Problemas encontrados y ajustes</h2>
	
	    <ul>
	        <li>
	            <strong>Desajuste entre sistemas de coordenadas:</strong> OpenCV utiliza un marco óptico (X: derecha, Y: abajo, Z: hacia adelante), mientras que el simulador usa un marco estándar (X: adelante, Y: izquierda, Z: arriba). Se aplicaron rotaciones de -90º en Z y X para alinear ambos sistemas.
	        </li>
	        <li>
	            <strong>Estimación incorrecta del yaw:</strong> Inicialmente, el cálculo con <code>arctan2</code> devolvía resultados inconsistentes debido a los ejes mal alineados. Tras corregir los marcos de referencia, el ángulo de orientación se estimó correctamente.
	        </li>
	        <li>
	            <strong>Falsos positivos o detección múltiple de balizas:</strong> En ocasiones se detectaban varias balizas simultáneamente. Se optó por seleccionar la más cercana (mayor área proyectada) por ser la que proporciona menor error en <code>solvePnP</code>.
	        </li>
	    </ul>
	
    </div>

    <!-- Contenido del séptimo post -->
    <div id="post_Dia7_7" class="post-container" style="display: none;">
	    <h2> Conclusión y Resultados Finales </h2>
	
	    <p>
	        Al finalizar el desarrollo del sistema de localización visual con AprilTags, se ha logrado un sistema robusto capaz de estimar la posición del robot en el entorno simulador, combinando información visual y odométrica.
	    </p>
	
	    <p>
	        El sistema funciona de la siguiente forma:
	        <ul>
	            <li> Cuando se detecta una baliza AprilTag, se calcula la pose del robot mediante una cadena de transformaciones desde el marco de la baliza hasta el del robot.</li>
	            <li> Cuando no hay detección, se emplea la odometría del simulador para mantener una estimación aproximada de la posición.</li>
	        </ul>
	    </p>
	
	    <h3>Valoración del rendimiento</h3>
	    <ul>
	        <li><strong>Precisión visual:</strong> Alta cuando hay detección.</li>
	        <li><strong>Odometía:</strong> Introduce ruido progresivamente, pero útil a corto plazo.</li>
	        <li><strong>Recuperación:</strong> Muy buena al volver a detectar una baliza.</li>
	        <li><strong>Generalización:</strong> Compatible con simuladores y entornos reales con balizas conocidas.</li>
	    </ul>

	    <h3>Posibles mejoras</h3>
	    <p>
	        Existe un campo de mejora en el cálculo de odometría para reducir el error acumulado cuando no se encuentran balizas.
	    </p>
	
	    <h3>Vídeo demostrativo</h3>
	    <div style="text-align: center; margin: 30px 0;">
	        <iframe width="560" height="315" src="https://www.youtube.com/embed/iPVz_WGa9l0" frameborder="0" allowfullscreen></iframe>
	    </div>
	
	    <p>
	        En el vídeo se puede ver cómo el robot inicia con detección visual, pierde contacto con las balizas (pasando a odometría) y finalmente recupera su posición gracias a una nueva detección.
	    </p>
	
    </div>


    <script>
    function mostrarPost(categoria, num) {
        let categorias = ["holonomico", "ackerman"];
        
        // Ocultar todos los posts de ambas categorías
        categorias.forEach(cat => {
            for (let i = 1; i <= 9; i++) {
                let post = document.getElementById(`post_${cat}_${i}`);
                if (post) {
                    post.style.opacity = 0;
                    post.style.display = 'none';
                }
            }
        });

        // Mostrar el post seleccionado
        setTimeout(() => {
            let post = document.getElementById(`post_${categoria}_${num}`);
            if (post) {
                post.style.display = 'block';
                setTimeout(() => {
                    post.style.opacity = 1;
                    post.style.transform = 'translateY(0)';
                }, 50);
            }
        }, 200);
    }
</script>

    <!-- 📌 Footer al final de la página -->
    <footer>
        <p><a href="../index.html">Volver a la página principal</a></p>
    </footer>
</body>
</html>
