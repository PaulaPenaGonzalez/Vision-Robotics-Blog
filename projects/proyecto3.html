<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Localizaci√≥n del robot con AprilTags</title>
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <h1> Localizaci√≥n del robot utilizando AprilTags </h1>
        <p>Estimaci√≥n de la pose del robot a partir de visi√≥n artificial y odometr√≠a.</p>
    </header>

    <!-- Introducci√≥n antes de las publicaciones -->
    <div class="container">
        <p>
        En este proyecto hemos implementado un sistema de localizaci√≥n de un robot m√≥vil mediante el uso de balizas AprilTags y t√©cnicas de visi√≥n por computador.
        </p>
        <p>
        A partir de las im√°genes capturadas por la c√°mara del robot, se detectan las balizas y se calcula la posici√≥n y orientaci√≥n del robot con respecto al mundo aplicando transformaciones geom√©tricas (roto-traslaciones) entre diferentes sistemas de referencia: c√°mara, baliza, robot y mundo.
        </p>
        <p>
        Cuando el robot no es capaz de ver ninguna baliza, el sistema recurre a la odometr√≠a para estimar su nueva posici√≥n, sumando los peque√±os desplazamientos detectados respecto a la √∫ltima posici√≥n conocida.
        </p>
        <p>
        Este enfoque h√≠brido permite mantener una localizaci√≥n continua y razonablemente precisa incluso en entornos parcialmente observables.
        </p>

        <!-- Imagen destacada del proyecto -->
        <div style="text-align: center; margin-top: 20px;">
            <img src="https://raw.githubusercontent.com/PaulaPenaGonzalez/Vision-Robotics-Blog/main/assets/images/proyecto3.png" 
                 alt="Localizaci√≥n del robot con AprilTags" 
                 style="width: 600px; height: auto; border-radius: 8px;">
        </div>
    </div>


    <div class="container">
        <h2>Publicaciones - Reconstrucci√≥n 3D</h2>
        <button onclick="mostrarPost('Dia1', 1)"> Primer post </button>
        <button onclick="mostrarPost('Dia2', 2)"> Segundo post</button>
        <button onclick="mostrarPost('Dia3', 3)"> Tercer post</button>
        <button onclick="mostrarPost('Dia4', 4)"> Cuarto post</button>
	<button onclick="mostrarPost('Dia5', 5)"> Quinto post</button>
	<button onclick="mostrarPost('Dia6', 6)"> Sexto post</button>
	<button onclick="mostrarPost('Dia7', 7)"> S√©ptimo post</button>
    </div>



    <!-- Contenido del primer post -->
    <div id="post_Dia1_1" class="post-container" style="display: none;">
	    <h2>Objetivo del proyecto y detecci√≥n de balizas AprilTags</h2>
	    
	    <h3> Objetivo principal</h3>
	    <p>
	        El objetivo de este proyecto es implementar un sistema de localizaci√≥n aut√≥noma de un robot m√≥vil en un entorno cerrado utilizando balizas AprilTags como referencias visuales. 
	    </p>
	    <p>
	        Para lograrlo, se emplea una c√°mara montada sobre el robot y se calcula su posici√≥n y orientaci√≥n (pose) a partir de la detecci√≥n y an√°lisis de las balizas. 
	        Cuando las balizas no est√°n visibles, se recurre a los datos de odometr√≠a para mantener una estimaci√≥n continua.
	    </p>
	    
	    <h3> Detecci√≥n de AprilTags</h3>
	    <p>
	        Se utiliza la biblioteca <code>pyapriltags</code> para detectar las balizas en las im√°genes capturadas por la c√°mara del robot. Esta herramienta proporciona las coordenadas de las esquinas de cada baliza y su identificador √∫nico.
	    </p>
	    <p>
	        A partir de estas esquinas, se aplica el algoritmo <code>solvePnP</code> de OpenCV para estimar la pose relativa de la ca√°mara con respecto a la baliza. Esta informaci√≥n, combinada con la pose conocida de la baliza en el mundo, permite calcular la pose del robot mediante una serie de roto-traslaciones.
	    </p>
	    
	    <div style="text-align: center; margin-top: 20px;">
	        <img src="https://raw.githubusercontent.com/PaulaPenaGonzalez/Vision-Robotics-Blog/main/assets/images/apriltags.png" 
	             alt="Detecci√≥n de AprilTags" 
	             style="width: 500px; height: auto; border-radius: 8px;">
	    </div>
	
	    <h3> Elecci√≥n de la baliza m√°s confiable</h3>
	    <p>
	        En cada imagen se pueden detectar m√∫ltiples balizas, pero no todas ofrecen la misma fiabilidad. 
	        Para seleccionar la mejor, se calcula el √°rea proyectada de cada baliza en la imagen, y se elige aquella con mayor √°rea aparente, ya que es la m√°s cercana y ofrece menor error en la estimaci√≥n de la pose.
	    </p>
	    <p>
	        Este criterio permite mejorar la precisi√≥n del sistema y evitar errores derivados de detecciones lejanas o parciales.
	    </p>
    </div>

    <!-- Contenido del segundo post -->
    <div id="post_Dia2_2" class="post-container" style="display: none;">
	    <h2>Estimaci√≥n de la pose del robot mediante transformaciones</h2>
	    
	    <h3> Transformaciones homog√©neas</h3>
	    <p>
	        A partir de la detecci√≥n de una baliza AprilTag, se puede calcular la posici√≥n y orientaci√≥n del robot respecto al mundo utilizando una serie de matrices de transformaci√≥n homog√©neas 4x4. 
	        El procedimiento se basa en transformar desde el sistema de referencia de la baliza hasta el del robot, pasando por la c√°mara.
	    </p>
	
	    <p>
	        La secuencia de transformaciones aplicada es la siguiente:
	    </p>
	
	    <pre>
	    T_robot_to_world = T_tag_to_world ¬∑ T_rot ¬∑ T_cam_to_tag ¬∑ T_rot_inv ¬∑ T_robot_to_cam
	    </pre>
	
	    <ul>
	        <li><strong>T_tag_to_world:</strong> Transformaci√≥n conocida desde el archivo YAML que define la posici√≥n de la baliza en el mundo.</li>
	        <li><strong>T_cam_to_tag:</strong> Se calcula <code>T_tag_to_cam</code> con <code>solvePnP</code> a partir de las esquinas detectadas y realizando la inversa obtenemos <code>T_cam_to_tag</code>.</li>
	        <li><strong>T_robot_to_cam:</strong> En el archivo .SDF, define la posici√≥n relativa de la c√°mara respecto al centro del robot. Se invierte para obtener el robot con respecto a la c√°mara. </li>
		<li><strong>T_rot:</strong> Define la rotaci√≥n de los ejes para pasar del sistema de referencia del simulador al marco √≥ptico (Open CV). </li>
	    </ul>
	
	    <h3> Extracci√≥n de la pose del robot</h3>
	    <p>
	        Una vez calculada la matriz <code>T_robot_to_world</code>, se extrae la posici√≥n del robot de la √∫ltima columna de la matriz (x, y), y su orientaci√≥n (yaw) a partir de los elementos de rotaci√≥n con la f√≥rmula:
	    </p>
	
	    <pre>
	    pitch = np.arctan2(-R_f[2, 0], np.sqrt(R_f[0, 0]**2 + R_f[1, 0]**2))
            yaw = np.arctan2(R_f[1, 0] / np.cos(pitch), R_f[0, 0] / np.cos(pitch))
	    </pre>
	
	    <p>
	        Esto nos da la orientaci√≥n del robot en radianes respecto al eje Z del mundo, completando as√≠ su pose: (x, y, yaw).
	    </p>
	
	    <div style="text-align: center; margin-top: 20px;">
	        <img src="https://raw.githubusercontent.com/TU_USUARIO/TU_REPO/main/assets/images/transformaciones_robot_pose.png" 
	             alt="Transformaciones para obtener la pose del robot" 
	             style="width: 520px; height: auto; border-radius: 8px;">
	    </div>
	
	    <h3> Consideraci√≥n del sistema de referencia √≥ptico</h3>
	    <p>
	        Como <code>solvePnP</code> devuelve la pose en el sistema de referencia √≥ptico de la c√°mara (donde X va a la derecha, Y hacia abajo y Z hacia delante), se aplica una conversi√≥n a sistema est√°ndar para que las transformaciones sean coherentes con la referencia del mundo y el robot. Esto se realiza mediante una rotaci√≥n compuesta:
	    </p>
	
	    <pre>
	    R_optical_to_standard = RotX(-90¬∞) ¬∑ RotZ(-90¬∞)
	    T_rot = np.eye(4)
            T_rot[:3, :3] = R_world_to_optical
	    </pre>
	
	    <p>
	        Esta rotaci√≥n se incorpora directamente y, de la misma manera, tambi√©n de forma inversa, en la multiplicaci√≥n final: idea y vuelta a coordenadas del marco √≥ptico (Open CV).
	    </p>
    </div>


    <!-- Contenido del tercer post -->
    <div id="post_Dia3_3" class="post-container" style="display: none;">
	<h2> Detecci√≥n y selecci√≥n de puntos caracter√≠sticos </h2>
        
	<p>Uno de los pilares de la reconstrucci√≥n 3D a partir de visi√≥n est√©reo es la <strong>geometr√≠a epipolar</strong>. En este post abordamos c√≥mo obtener la l√≠nea epipolar en la imagen derecha a partir de un punto caracter√≠stico detectado en la imagen izquierda.</p>

	<div style="text-align: center; margin-top: 20px;">
	        <img src="https://raw.githubusercontent.com/PaulaPenaGonzalez/Vision-Robotics-Blog/main/assets/images/TRIANGULACION.png" 
	             alt="Reconstrucci√≥n 3D a partir de bordes" 
	             style="width: 350px; height: auto; border-radius: 8px;">
    	</div>
	
	<h3> Retroproyecci√≥n del punto izquierdo </h3>

	<p>Cada punto caracter√≠stico extra√≠do de la imagen izquierda es transformado a coordenadas del mundo real mediante retroproyecci√≥n. Para ello, se utiliza la matriz de proyecci√≥n inversa de la c√°mara izquierda, proporcionada por la herramienta <code>HAL.backproject</code>.</p>
        
        <h3>Vector director del rayo</h3>
        <p>Con el punto 3D obtenido y la posici√≥n de la c√°mara izquierda, se calcula el vector director del rayo de retroproyecci√≥n:</p>
    	<p><code>u = punto_3d - centro_c√°mara</code></p>
    	<p>Este vector se normaliza para definir una direcci√≥n en el espacio desde el centro de la c√°mara.</p>
        
        <h3>Proyecci√≥n del rayo en la imagen derecha</h3>

	<p>Se seleccionan dos puntos arbitrarios sobre el rayo en 3D (por ejemplo, a cierta distancia a lo largo del vector director) y se proyectan sobre la imagen derecha utilizando la funci√≥n <code>HAL.project</code>.</p>
	<p>Posteriormente, se convierten estas coordenadas proyectadas de espacio √≥ptico a espacio gr√°fico con <code>HAL.opticalToGrafic</code>.</p>
	
	<h3> C√°lculo de la l√≠nea epipolar</h3>
	<p>Una vez tenemos los dos puntos proyectados en la imagen derecha, la l√≠nea epipolar se calcula como la recta que pasa por ambos:</p>
	<p><code>line = np.cross(p1_2d, p2_2d)</code></p>
	<p>Esto nos da los coeficientes <code>(a, b, c)</code> de la recta epipolar en la forma <code>ax + by + c = 0</code>, que se dibujar√° sobre la imagen derecha para restringir la b√∫squeda de hom√≥logos a esa l√≠nea.</p>

	<div style="text-align: center; margin-top: 20px;">
	        <img src="https://raw.githubusercontent.com/PaulaPenaGonzalez/Vision-Robotics-Blog/main/assets/images/RectaEpipolar.jpg" 
	             alt="Reconstrucci√≥n 3D a partir de bordes" 
	             style="width: 300px; height: auto; border-radius: 8px;">
    	</div>
	
    </div>

    <!-- Contenido del cuarto post -->
    <div id="post_Dia4_4" class="post-container" style="display: none;">
	
	<h2>Emparejamiento por correlaci√≥n (b√∫squeda de hom√≥logos)</h2>

	<p>Una vez definida la l√≠nea epipolar en la imagen derecha, el siguiente paso es buscar el punto hom√≥logo correspondiente al punto de la imagen izquierda. Esta tarea se realiza mediante correlaci√≥n de ventanas y t√©cnicas de comparaci√≥n basadas en diferencias de intensidad.</p>
	
        <h3>Funci√≥n <code>buscar_match_epipolar_Canny()</code></h3>

	<p>Se dise√±√≥ una funci√≥n que recorre la l√≠nea epipolar en la imagen derecha evaluando m√∫ltiples coordenadas posibles, comparando cada una con la regi√≥n del punto original en la imagen izquierda.</p>

        <ul>
	        <li>Para cada valor de <code>x</code> a lo largo de la l√≠nea epipolar, se calcula el correspondiente <code>y</code> usando la ecuaci√≥n de la recta <code>y = -(a¬∑x + c)/b</code>.</li>
	        <li>Se extrae una ventana centrada en <code>(x, y)</code> en la imagen derecha y se compara con una ventana de mismo tama√±o centrada en el punto original de la imagen izquierda.</li>
	        <li>La comparaci√≥n se realiza usando el error <strong>SSD normalizado</strong> (Sum of Squared Differences), lo cual penaliza las diferencias de intensidad.</li>
    	</ul> 

	<p>Una vez recorrida toda la l√≠nea epipolar, se selecciona el punto con menor SSD como mejor candidato, siempre que est√© por debajo de un umbral que garantiza una buena correspondencia.</p>

	<h3>Filtro adicional por color</h3>
    	<p>Para mejorar a√∫n m√°s la fiabilidad de los matches, se quiso implementar un filtro adicional que compara el color del p√≠xel en la imagen izquierda con el color del p√≠xel candidato en la derecha.</p>

	<ul>
		<li>Se calcula la distancia eucl√≠dea entre ambos colores (en espacio RGB).</li>
	        <li>Si la diferencia es demasiado alta (por encima de un umbral establecido), se descarta el match aunque tenga un buen SSD.</li>
	</ul>

	<p>Este enfoque combinado de <strong>correlaci√≥n espacial</strong> y <strong>consistencia de color</strong> hubiese podido mejorar significativamente la robustez del emparejamiento, reduciendo falsos positivos y contribuyendo a una nube de puntos 3D m√°s precisa. Sin embargo en el modelo final, solo se implement√≥ la correlaci√≥n espacial.</p>

	<p>A continuaci√≥n se muestra un video del proceso de correlaci√≥n en ambas im√°genes:</p>

	

    </div>

    <!-- Contenido del quinto post -->
    <div id="post_Dia5_5" class="post-container" style="display: none;">
	
	<h2>Triangulaci√≥n del punto 3D</h2>
	    
	<p>Una vez identificada una pareja de p√≠xeles correspondientes (uno en cada imagen), el siguiente paso es estimar la posici√≥n 3D del punto de la escena que proyecta en ambas c√°maras. Para ello, utilizamos una t√©cnica basada en la geometr√≠a de retroproyecci√≥n.</p>

	<h3>Rayos de retroproyecci√≥n</h3>
  	<p>Cada p√≠xel, al ser retroproyectado mediante la matriz de calibraci√≥n y la posici√≥n de la c√°mara, genera un rayo que apunta desde el centro √≥ptico hacia la escena. Ambos rayos (izquierdo y derecho) deber√≠an, idealmente, cruzarse en el espacio tridimensional.</p>

	<h3>Implementaci√≥n de <code>triangulate_midpoint()</code></h3>
	<ul>
	    <li>Se plante√≥ y resolvi√≥ un sistema lineal para encontrar los puntos m√°s cercanos entre ambos rayos.</li>
	    <li>Estos puntos se denominan <code>P1</code> y <code>P2</code>, uno sobre cada rayo.</li>
	    <li>El punto 3D estimado es el punto medio entre ambos: <strong>el centro del segmento m√≠nimo entre rayos</strong>.</li>
	</ul>

	<div style="text-align: center; margin: 30px 0;">
	<iframe width="560" height="315" src="https://www.youtube.com/embed/Fd0WqJa0TB8" frameborder="0" allowfullscreen></iframe>
	</div>

	<h3>Filtrado de puntos no fiables (outliers)</h3>
	<ul>
	    <li>Si la distancia entre los rayos (<code>‚ÄñP1 - P2‚Äñ</code>) es demasiado alta, el punto se descarta por inconsistencia geom√©trica.</li>
	    <li>Tambi√©n se descartan las soluciones en las que los par√°metros del sistema lineal (<code>t</code> o <code>s</code>) resultan negativos. Esto indica que el punto estar√≠a "detr√°s" de alguna de las c√°maras, lo cual no tiene sentido f√≠sico.</li>
	</ul>

	<p>Con esta funci√≥n, se garantiza que cada punto de la nube reconstruida tiene una base s√≥lida desde el punto de vista geom√©trico y espacial. Solo se conservan aquellos puntos que est√°n bien triangulados y situados en la regi√≥n visual v√°lida de ambas c√°maras.</p>
	
    </div>

    <!-- Contenido del sexto post -->
    <div id="post_Dia6_6" class="post-container" style="display: none;">
        <h2> Proyecci√≥n y visualizaci√≥n </h2>
	
        <p>Tras obtener la posici√≥n 3D de un punto mediante triangulaci√≥n, el siguiente paso fue representarlo de forma visual dentro del entorno. Para ello, se emple√≥ el visor 3D de Unibotics como herramienta de visualizaci√≥n.</p>

	<h3>Conversi√≥n de unidades</h3>
  	<p>Como las posiciones reconstruidas est√°n en mil√≠metros (debido a la escala de las c√°maras), fue necesario dividir por 100 para convertirlas a dec√≠metros. Esto permite una visualizaci√≥n adecuada y proporcional dentro del entorno virtual.</p>

	<h3>Color de los puntos</h3>
	<p>Cada punto 3D se visualiza no solo por su posici√≥n, sino tambi√©n por el color extra√≠do del p√≠xel izquierdo original. Este detalle aporta riqueza visual a la nube de puntos, ya que cada uno conserva la tonalidad del objeto real que lo origin√≥.</p>
	<ul>
	    <li>Se accede al color del p√≠xel en la imagen izquierda (usualmente en formato BGR).</li>
	    <li>Se convierte a RGB para usarlo correctamente en el visor.</li>
	    <li>Se construye una tupla con la forma <code>(x, y, z, r, g, b)</code>.</li>
	</ul>

	<h3>Visualizaci√≥n en el entorno 3D</h3>
  	<p>Con la posici√≥n ajustada y el color asociado, cada punto se env√≠a al visor con la funci√≥n <code>GUI.ShowNewPoints()</code>. As√≠, la nube de puntos 3D se va formando de forma interactiva, punto a punto, representando fielmente la geometr√≠a y el aspecto de la escena original.</p>

  	<p>Este paso permite validar visualmente la reconstrucci√≥n y ajustar par√°metros si se detectan errores en la escala, el color o la alineaci√≥n de los puntos generados.</p>

    </div>

    <!-- Contenido del s√©ptimo post -->
    <div id="post_Dia7_7" class="post-container" style="display: none;">
        <h2> Limpieza y refinamiento </h2>
        <p>Una vez generada la nube de puntos 3D, el siguiente paso fundamental fue su depuraci√≥n. La reconstrucci√≥n puede incluir puntos err√≥neos u outliers debido a emparejamientos incorrectos, ruido o geometr√≠as mal trianguladas.</p>

	<h3>Filtrado por distancia entre rayos</h3>
 	<p>Durante la triangulaci√≥n, se calcul√≥ la distancia entre los rayos de retroproyecci√≥n de cada c√°mara. Si esta distancia era demasiado grande, se consideraba que los rayos no se cruzaban de forma fiable, y el punto 3D correspondiente se descartaba.</p>

	<h3>Umbral sobre los par√°metros de triangulaci√≥n <code>t</code> y <code>s</code></h3>
	<p> Otro criterio de validaci√≥n consisti√≥ en comprobar que los par√°metros <code>t</code> y <code>s</code> obtenidos durante la resoluci√≥n del sistema de triangulaci√≥n fueran positivos y estuvieran dentro de un rango aceptable.</p>
	<p>Estos valores determinan la posici√≥n del punto a lo largo de los rayos, por lo que si son negativos o extremadamente grandes, significa que el punto reconstruido estar√≠a fuera del campo visual de la escena. </p>
	
	<p> Gracias a la incorporaci√≥n de estos dos filtros, la nube de puntos final presenta una geometr√≠a mucho m√°s coherente y limpia,facilitando su an√°lisis visual y reduciendo significativamente los outliers.</p>

	<p>A cotinuaci√≥n se muestra un video con el resultado final de la reconstrucci√≥n 3D:</p>
	
	<div style="text-align: center; margin: 30px 0;">
	<iframe width="560" height="315" src="https://www.youtube.com/embed/ceEEqg4W2bo" frameborder="0" allowfullscreen></iframe>
	</div>
    
    </div>

    <script>
    function mostrarPost(categoria, num) {
        let categorias = ["holonomico", "ackerman"];
        
        // Ocultar todos los posts de ambas categor√≠as
        categorias.forEach(cat => {
            for (let i = 1; i <= 9; i++) {
                let post = document.getElementById(`post_${cat}_${i}`);
                if (post) {
                    post.style.opacity = 0;
                    post.style.display = 'none';
                }
            }
        });

        // Mostrar el post seleccionado
        setTimeout(() => {
            let post = document.getElementById(`post_${categoria}_${num}`);
            if (post) {
                post.style.display = 'block';
                setTimeout(() => {
                    post.style.opacity = 1;
                    post.style.transform = 'translateY(0)';
                }, 50);
            }
        }, 200);
    }
</script>

    <!-- üìå Footer al final de la p√°gina -->
    <footer>
        <p><a href="../index.html">Volver a la p√°gina principal</a></p>
    </footer>
</body>
</html>
