<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Proyecto 1 - Detecci√≥n de Bordes</title>
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <h1> Reconstrucci√≥n 3D de una escena 2D </h1>
        <p> Progreso y avances de este proyecto. </p>
    </header>

    <!-- Introducci√≥n antes de las publicaciones -->
    <div class="container">
    	<p>
        En este proyecto, se ha desarrollado un sistema de reconstrucci√≥n 3D a partir de im√°genes est√©reo. A trav√©s de t√©cnicas cl√°sicas de visi√≥n por computador, como la geometr√≠a epipolar, la correlaci√≥n de ventanas y la triangulaci√≥n de rayos, se genera una nube de puntos coloreada que representa la escena capturada por las c√°maras. 
    	</p>
    	<p>
        A continuaci√≥n, puedes leer las publicaciones relacionadas con este proyecto, donde se detallan los avances, problemas encontrados y soluciones aplicadas. 
	</p>

	<!-- Imagen de la reconstrucci√≥n -->
    	<div style="text-align: center; margin-top: 20px;">
	        <img src="https://raw.githubusercontent.com/PaulaPenaGonzalez/Vision-Robotics-Blog/main/assets/images/proyecto2.jpg" 
	             alt="Reconstrucci√≥n 3D a partir de bordes" 
	             style="width: 600px; height: auto; border-radius: 8px;">
    	</div>
    </div>

    <div class="container">
        <h2>Publicaciones - Reconstrucci√≥n 3D</h2>
        <button onclick="mostrarPost('Dia1', 1)"> Primer post </button>
        <button onclick="mostrarPost('Dia2', 2)"> Segundo post</button>
        <button onclick="mostrarPost('Dia3', 3)"> Tercer post</button>
        <button onclick="mostrarPost('Dia4', 4)"> Cuarto post</button>
	<button onclick="mostrarPost('Dia5', 5)"> Quinto post</button>
	<button onclick="mostrarPost('Dia6', 6)"> Sexto post</button>
	<button onclick="mostrarPost('Dia7', 7)"> S√©ptimo post</button>
    </div>

    <!-- Contenido del primer post -->
    <div id="post_Dia1_1" class="post-container" style="display: none;">
	<h2> Preparaci√≥n inicial y lectura de im√°genes </h2>
	<h3>Objetivo:</h3>
    	<p> Cargar y preprocesar las im√°genes est√©reo capturadas por las dos c√°maras calibradas (izquierda y derecha). Fundamental para extraer la informaci√≥n visual relevante que servir√° como base para todo el sistema de reconstrucci√≥n 3D.</p>

    	<h3> Captura de Im√°genes</h3>

    	<p>Las im√°genes izquierda y derecha fueron obtenidas desde el simulador Unibotics. Estas im√°genes corresponden a la misma escena vista desde dos √°ngulos ligeramente diferentes, lo que permite generar percepci√≥n de profundidad gracias al paralaje.</p>

	<div style="text-align: center; margin-top: 20px;">
	        <img src="https://raw.githubusercontent.com/PaulaPenaGonzalez/Vision-Robotics-Blog/main/assets/images/ImagenLImagenR.jpg" 
	             alt="Reconstrucci√≥n 3D a partir de bordes" 
	             style="width: 350px; height: auto; border-radius: 8px;">
    	</div>
	    
    	<h3> Conversi√≥n a Escala de Grises</h3>
    	<p> Para simplificar el an√°lisis visual y reducir la carga computacional, se realiz√≥ una conversi√≥n de las im√°genes a escala de grises. Esto permite centrarse en la estructura de la escena sin que el color interfiera en las operaciones iniciales como la detecci√≥n de bordes.</p>

    	<h3>Detecci√≥n de Bordes</h3>
	<p>La etapa clave en este punto fue aplicar un detector de bordes, espec√≠ficamente el m√©todo de Canny. Esta t√©cnica resalta los contornos y l√≠neas m√°s relevantes de la escena, permitiendo identificar los puntos caracter√≠sticos que luego se utilizar√°n para hallar correspondencias entre ambas im√°genes. Se hizo un ajuste manual de los par√°metros del detector para encontrar un equilibrio entre sensibilidad (captar todos los bordes importantes) y robustez (evitar bordes producidos por ruido o detalles irrelevantes).</p>

	<div style="text-align: center; margin-top: 20px;">
	        <img src="https://raw.githubusercontent.com/PaulaPenaGonzalez/Vision-Robotics-Blog/main/assets/images/Bordes.jpg" 
	             alt="Reconstrucci√≥n 3D a partir de bordes" 
	             style="width: 350px; height: auto; border-radius: 8px;">
    	</div>

    	<h3>Visualizaci√≥n y Verificaci√≥n</h3>
    	<p>Una vez obtenidas las m√°scaras de bordes, se visualizaron ambas im√°genes con sus respectivos contornos resaltados. Esto ayud√≥ a comprobar que los par√°metros elegidos proporcionaban una buena base para continuar con la reconstrucci√≥n.</p>
    
    </div>

    <!--Contenido del segundo post -->
    <div id="post_Dia2_2" class="post-container" style="display: none;">
        <h2> ¬øQu√© significa detectar puntos caracter√≠sticos? </h2>
        <p>En el proceso de reconstrucci√≥n 3D, identificar los puntos m√°s relevantes de una imagen es esencial. No todos los p√≠xeles aportan informaci√≥n √∫til: necesitamos centrarnos en aquellos que definen contornos, estructuras y geometr√≠as bien marcadas en la escena. En este post, abordamos c√≥mo se extraen estos puntos a partir de una imagen ya procesada por un detector de bordes.</p>
        
        <h3> Selecci√≥n a partir de bordes detectados </h3>
	<p>Una vez obtenida la m√°scara de bordes mediante el detector de Canny, lo que tenemos es una imagen en blanco y negro donde los bordes aparecen marcados en blanco (valor alto).</p>
	<p>A partir de aqu√≠, el objetivo es localizar las coordenadas de estos p√≠xeles blancos que representan los contornos m√°s significativos.</p>
	
	<p>Para ello, se recorre la imagen buscando todas las posiciones en las que el valor del p√≠xel supere cierto umbral (en este caso, simplemente donde el valor es mayor que cero). </p>
	<p>Esto nos devuelve las coordenadas (x, y) de los bordes relevantes. As√≠ se construye una lista de puntos que actuar√°n como candidatos para buscar correspondencias entre im√°genes izquierda y derecha.</p>
        
        <h3> Un peque√±o extra </h3>
        <p>Durante las pruebas, se decidi√≥ no procesar todos los puntos a la vez, sino seleccionar uno de cada pocos (por ejemplo, cada tres puntos). </p>
	<p>Esto ayuda a mantener el rendimiento y obtener una nube de puntos m√°s limpia y distribuida de forma uniforme. Con esta lista de puntos caracter√≠stica ya lista, estamos preparados para pasar al siguiente paso: proyectar cada uno de ellos y buscar su hom√≥logo en la segunda imagen </p>
        
    </div>

    <!-- Contenido del tercer post -->
    <div id="post_Dia3_3" class="post-container" style="display: none;">
	<h2> Detecci√≥n y selecci√≥n de puntos caracter√≠sticos </h2>
        
	<p>Uno de los pilares de la reconstrucci√≥n 3D a partir de visi√≥n est√©reo es la <strong>geometr√≠a epipolar</strong>. En este post abordamos c√≥mo obtener la l√≠nea epipolar en la imagen derecha a partir de un punto caracter√≠stico detectado en la imagen izquierda.</p>

	<div style="text-align: center; margin-top: 20px;">
	        <img src="https://raw.githubusercontent.com/PaulaPenaGonzalez/Vision-Robotics-Blog/main/assets/images/TRIANGULACION.png" 
	             alt="Reconstrucci√≥n 3D a partir de bordes" 
	             style="width: 350px; height: auto; border-radius: 8px;">
    	</div>
	
	<h3> Retroproyecci√≥n del punto izquierdo </h3>

	<p>Cada punto caracter√≠stico extra√≠do de la imagen izquierda es transformado a coordenadas del mundo real mediante retroproyecci√≥n. Para ello, se utiliza la matriz de proyecci√≥n inversa de la c√°mara izquierda, proporcionada por la herramienta <code>HAL.backproject</code>.</p>
        
        <h3>Vector director del rayo</h3>
        <p>Con el punto 3D obtenido y la posici√≥n de la c√°mara izquierda, se calcula el vector director del rayo de retroproyecci√≥n:</p>
    	<p><code>u = punto_3d - centro_c√°mara</code></p>
    	<p>Este vector se normaliza para definir una direcci√≥n en el espacio desde el centro de la c√°mara.</p>
        
        <h3>Proyecci√≥n del rayo en la imagen derecha</h3>

	<p>Se seleccionan dos puntos arbitrarios sobre el rayo en 3D (por ejemplo, a cierta distancia a lo largo del vector director) y se proyectan sobre la imagen derecha utilizando la funci√≥n <code>HAL.project</code>.</p>
	<p>Posteriormente, se convierten estas coordenadas proyectadas de espacio √≥ptico a espacio gr√°fico con <code>HAL.opticalToGrafic</code>.</p>
	
	<h3> C√°lculo de la l√≠nea epipolar</h3>
	<p>Una vez tenemos los dos puntos proyectados en la imagen derecha, la l√≠nea epipolar se calcula como la recta que pasa por ambos:</p>
	<p><code>line = np.cross(p1_2d, p2_2d)</code></p>
	<p>Esto nos da los coeficientes <code>(a, b, c)</code> de la recta epipolar en la forma <code>ax + by + c = 0</code>, que se dibujar√° sobre la imagen derecha para restringir la b√∫squeda de hom√≥logos a esa l√≠nea.</p>

	<div style="text-align: center; margin-top: 20px;">
	        <img src="https://raw.githubusercontent.com/PaulaPenaGonzalez/Vision-Robotics-Blog/main/assets/images/RectaEpipolar.jpg" 
	             alt="Reconstrucci√≥n 3D a partir de bordes" 
	             style="width: 300px; height: auto; border-radius: 8px;">
    	</div>
	
    </div>

    <!-- Contenido del cuarto post -->
    <div id="post_Dia4_4" class="post-container" style="display: none;">
	
	<h2>Emparejamiento por correlaci√≥n (b√∫squeda de hom√≥logos)</h2>

	<p>Una vez definida la l√≠nea epipolar en la imagen derecha, el siguiente paso es buscar el punto hom√≥logo correspondiente al punto de la imagen izquierda. Esta tarea se realiza mediante correlaci√≥n de ventanas y t√©cnicas de comparaci√≥n basadas en diferencias de intensidad.</p>
	
        <h3>Funci√≥n <code>buscar_match_epipolar_Canny()</code></h3>

	<p>Se dise√±√≥ una funci√≥n que recorre la l√≠nea epipolar en la imagen derecha evaluando m√∫ltiples coordenadas posibles, comparando cada una con la regi√≥n del punto original en la imagen izquierda.</p>

        <ul>
	        <li>Para cada valor de <code>x</code> a lo largo de la l√≠nea epipolar, se calcula el correspondiente <code>y</code> usando la ecuaci√≥n de la recta <code>y = -(a¬∑x + c)/b</code>.</li>
	        <li>Se extrae una ventana centrada en <code>(x, y)</code> en la imagen derecha y se compara con una ventana de mismo tama√±o centrada en el punto original de la imagen izquierda.</li>
	        <li>La comparaci√≥n se realiza usando el error <strong>SSD normalizado</strong> (Sum of Squared Differences), lo cual penaliza las diferencias de intensidad.</li>
    	</ul> 

	<p>Una vez recorrida toda la l√≠nea epipolar, se selecciona el punto con menor SSD como mejor candidato, siempre que est√© por debajo de un umbral que garantiza una buena correspondencia.</p>

	<h3>Filtro adicional por color</h3>
    	<p>Para mejorar a√∫n m√°s la fiabilidad de los matches, se quiso implementar un filtro adicional que compara el color del p√≠xel en la imagen izquierda con el color del p√≠xel candidato en la derecha.</p>

	<ul>
		<li>Se calcula la distancia eucl√≠dea entre ambos colores (en espacio RGB).</li>
	        <li>Si la diferencia es demasiado alta (por encima de un umbral establecido), se descarta el match aunque tenga un buen SSD.</li>
	</ul>

	<p>Este enfoque combinado de <strong>correlaci√≥n espacial</strong> y <strong>consistencia de color</strong> hubiese podido mejorar significativamente la robustez del emparejamiento, reduciendo falsos positivos y contribuyendo a una nube de puntos 3D m√°s precisa. Sin embargo en el modelo final, solo se implement√≥ la correlaci√≥n espacial.</p>

	<p>A continuaci√≥n se muestra un video del proceso de correlaci√≥n en ambas im√°genes:</p>

	

    </div>

    <!-- Contenido del quinto post -->
    <div id="post_Dia5_5" class="post-container" style="display: none;">
	
	<h2>Triangulaci√≥n del punto 3D</h2>
	    
	<p>Una vez identificada una pareja de p√≠xeles correspondientes (uno en cada imagen), el siguiente paso es estimar la posici√≥n 3D del punto de la escena que proyecta en ambas c√°maras. Para ello, utilizamos una t√©cnica basada en la geometr√≠a de retroproyecci√≥n.</p>

	<h3>Rayos de retroproyecci√≥n</h3>
  	<p>Cada p√≠xel, al ser retroproyectado mediante la matriz de calibraci√≥n y la posici√≥n de la c√°mara, genera un rayo que apunta desde el centro √≥ptico hacia la escena. Ambos rayos (izquierdo y derecho) deber√≠an, idealmente, cruzarse en el espacio tridimensional.</p>

	<h3>Implementaci√≥n de <code>triangulate_midpoint()</code></h3>
	<ul>
	    <li>Se plante√≥ y resolvi√≥ un sistema lineal para encontrar los puntos m√°s cercanos entre ambos rayos.</li>
	    <li>Estos puntos se denominan <code>P1</code> y <code>P2</code>, uno sobre cada rayo.</li>
	    <li>El punto 3D estimado es el punto medio entre ambos: <strong>el centro del segmento m√≠nimo entre rayos</strong>.</li>
	</ul>

	<div style="text-align: center; margin: 30px 0;">
	<iframe width="560" height="315" src="https://www.youtube.com/embed/Fd0WqJa0TB8" frameborder="0" allowfullscreen></iframe>
	</div>

	<h3>Filtrado de puntos no fiables (outliers)</h3>
	<ul>
	    <li>Si la distancia entre los rayos (<code>‚ÄñP1 - P2‚Äñ</code>) es demasiado alta, el punto se descarta por inconsistencia geom√©trica.</li>
	    <li>Tambi√©n se descartan las soluciones en las que los par√°metros del sistema lineal (<code>t</code> o <code>s</code>) resultan negativos. Esto indica que el punto estar√≠a "detr√°s" de alguna de las c√°maras, lo cual no tiene sentido f√≠sico.</li>
	</ul>

	<p>Con esta funci√≥n, se garantiza que cada punto de la nube reconstruida tiene una base s√≥lida desde el punto de vista geom√©trico y espacial. Solo se conservan aquellos puntos que est√°n bien triangulados y situados en la regi√≥n visual v√°lida de ambas c√°maras.</p>
	
    </div>

    <!-- Contenido del sexto post -->
    <div id="post_Dia6_6" class="post-container" style="display: none;">
        <h2> Proyecci√≥n y visualizaci√≥n </h2>
	
        <p>Tras obtener la posici√≥n 3D de un punto mediante triangulaci√≥n, el siguiente paso fue representarlo de forma visual dentro del entorno. Para ello, se emple√≥ el visor 3D de Unibotics como herramienta de visualizaci√≥n.</p>

	<h3>Conversi√≥n de unidades</h3>
  	<p>Como las posiciones reconstruidas est√°n en mil√≠metros (debido a la escala de las c√°maras), fue necesario dividir por 100 para convertirlas a dec√≠metros. Esto permite una visualizaci√≥n adecuada y proporcional dentro del entorno virtual.</p>

	<h3>Color de los puntos</h3>
	<p>Cada punto 3D se visualiza no solo por su posici√≥n, sino tambi√©n por el color extra√≠do del p√≠xel izquierdo original. Este detalle aporta riqueza visual a la nube de puntos, ya que cada uno conserva la tonalidad del objeto real que lo origin√≥.</p>
	<ul>
	    <li>Se accede al color del p√≠xel en la imagen izquierda (usualmente en formato BGR).</li>
	    <li>Se convierte a RGB para usarlo correctamente en el visor.</li>
	    <li>Se construye una tupla con la forma <code>(x, y, z, r, g, b)</code>.</li>
	</ul>

	<h3>Visualizaci√≥n en el entorno 3D</h3>
  	<p>Con la posici√≥n ajustada y el color asociado, cada punto se env√≠a al visor con la funci√≥n <code>GUI.ShowNewPoints()</code>. As√≠, la nube de puntos 3D se va formando de forma interactiva, punto a punto, representando fielmente la geometr√≠a y el aspecto de la escena original.</p>

  	<p>Este paso permite validar visualmente la reconstrucci√≥n y ajustar par√°metros si se detectan errores en la escala, el color o la alineaci√≥n de los puntos generados.</p>

    </div>

    <!-- Contenido del s√©ptimo post -->
    <div id="post_Dia7_7" class="post-container" style="display: none;">
        <h2> Limpieza y refinamiento </h2>
        <p>Una vez generada la nube de puntos 3D, el siguiente paso fundamental fue su depuraci√≥n. La reconstrucci√≥n puede incluir puntos err√≥neos u outliers debido a emparejamientos incorrectos, ruido o geometr√≠as mal trianguladas.</p>

	<h3>Filtrado por distancia entre rayos</h3>
 	<p>Durante la triangulaci√≥n, se calcul√≥ la distancia entre los rayos de retroproyecci√≥n de cada c√°mara. Si esta distancia era demasiado grande, se consideraba que los rayos no se cruzaban de forma fiable, y el punto 3D correspondiente se descartaba.</p>

	<h3>Umbral sobre los par√°metros de triangulaci√≥n <code>t</code> y <code>s</code></h3>
	<p> Otro criterio de validaci√≥n consisti√≥ en comprobar que los par√°metros <code>t</code> y <code>s</code> obtenidos durante la resoluci√≥n del sistema de triangulaci√≥n fueran positivos y estuvieran dentro de un rango aceptable.</p>
	<p>Estos valores determinan la posici√≥n del punto a lo largo de los rayos, por lo que si son negativos o extremadamente grandes, significa que el punto reconstruido estar√≠a fuera del campo visual de la escena. </p>
	
	<p> Gracias a la incorporaci√≥n de estos dos filtros, la nube de puntos final presenta una geometr√≠a mucho m√°s coherente y limpia,facilitando su an√°lisis visual y reduciendo significativamente los outliers.</p>

	<p>A cotinuaci√≥n se muestra un video con el resultado final de la reconstrucci√≥n 3D:</p>
	
	<div style="text-align: center; margin: 30px 0;">
	<iframe width="560" height="315" src="https://www.youtube.com/embed/ceEEqg4W2bo" frameborder="0" allowfullscreen></iframe>
	</div>
    
    </div>

    <script>
    function mostrarPost(categoria, num) {
        let categorias = ["holonomico", "ackerman"];
        
        // Ocultar todos los posts de ambas categor√≠as
        categorias.forEach(cat => {
            for (let i = 1; i <= 9; i++) {
                let post = document.getElementById(`post_${cat}_${i}`);
                if (post) {
                    post.style.opacity = 0;
                    post.style.display = 'none';
                }
            }
        });

        // Mostrar el post seleccionado
        setTimeout(() => {
            let post = document.getElementById(`post_${categoria}_${num}`);
            if (post) {
                post.style.display = 'block';
                setTimeout(() => {
                    post.style.opacity = 1;
                    post.style.transform = 'translateY(0)';
                }, 50);
            }
        }, 200);
    }
</script>

    <!-- üìå Footer al final de la p√°gina -->
    <footer>
        <p><a href="../index.html">Volver a la p√°gina principal</a></p>
    </footer>
</body>
</html>
