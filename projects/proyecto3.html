<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Localizaci√≥n del robot con AprilTags</title>
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <h1> Localizaci√≥n del robot utilizando AprilTags </h1>
        <p>Estimaci√≥n de la pose del robot a partir de visi√≥n artificial y odometr√≠a.</p>
    </header>

    <!-- Introducci√≥n antes de las publicaciones -->
    <div class="container">
        <p>
        En este proyecto se ha implementado un sistema de localizaci√≥n de un robot m√≥vil mediante el uso de balizas AprilTags y t√©cnicas de visi√≥n por computador.
        </p>
        <p>
        A partir de las im√°genes capturadas por la c√°mara del robot, se detectan las balizas y se calcula la posici√≥n y orientaci√≥n del robot con respecto al mundo aplicando transformaciones geom√©tricas (roto-traslaciones) entre diferentes sistemas de referencia: c√°mara, baliza, robot y mundo.
        </p>
        <p>
        Cuando el robot no es capaz de ver ninguna baliza, el sistema recurre a la odometr√≠a para estimar su nueva posici√≥n.
        </p>

        <!-- Imagen destacada del proyecto -->
        <div style="text-align: center; margin-top: 20px;">
            <img src="https://raw.githubusercontent.com/PaulaPenaGonzalez/Vision-Robotics-Blog/main/assets/images/proyecto3.png" 
                 alt="Localizaci√≥n del robot con AprilTags" 
                 style="width: 600px; height: auto; border-radius: 8px;">
        </div>
    </div>


    <div class="container">
        <h2>Publicaciones - Reconstrucci√≥n 3D</h2>
        <button onclick="mostrarPost('Dia1', 1)"> Primer post </button>
        <button onclick="mostrarPost('Dia2', 2)"> Segundo post</button>
        <button onclick="mostrarPost('Dia3', 3)"> Tercer post</button>
        <button onclick="mostrarPost('Dia4', 4)"> Cuarto post</button>
	<button onclick="mostrarPost('Dia5', 5)"> Quinto post</button>
	<button onclick="mostrarPost('Dia6', 6)"> Sexto post</button>
	<button onclick="mostrarPost('Dia7', 7)"> S√©ptimo post</button>
    </div>



    <!-- Contenido del primer post -->
    <div id="post_Dia1_1" class="post-container" style="display: none;">
	    <h2>Objetivo del proyecto y detecci√≥n de balizas AprilTags</h2>
	    
	    <h3> Objetivo principal</h3>
	    <p>
	        El objetivo de este proyecto es implementar un sistema de localizaci√≥n aut√≥noma de un robot m√≥vil en un entorno cerrado utilizando balizas AprilTags como referencias visuales. 
	    </p>
	    <p>
	        Para lograrlo, se emplea una c√°mara montada sobre el robot y se calcula su posici√≥n y orientaci√≥n (pose) a partir de la detecci√≥n y an√°lisis de las balizas. 
	        Cuando las balizas no est√°n visibles, se recurre a los datos de odometr√≠a para mantener una estimaci√≥n continua.
	    </p>
	    
	    <h3> Detecci√≥n de AprilTags</h3>
	    <p>
	        Se utiliza la biblioteca <code>pyapriltags</code> para detectar las balizas en las im√°genes capturadas por la c√°mara del robot. Esta herramienta proporciona las coordenadas de las esquinas de cada baliza y su identificador √∫nico.
	    </p>
	    <p>
	        A partir de estas esquinas, se aplica el algoritmo <code>solvePnP</code> de OpenCV para estimar la pose relativa de la c√°mara con respecto a la baliza. Esta informaci√≥n, combinada con la pose conocida de la baliza en el mundo, permite calcular la pose del robot mediante una serie de roto-traslaciones.
	    </p>
	    
	    <div style="text-align: center; margin-top: 20px;">
	        <img src="https://raw.githubusercontent.com/PaulaPenaGonzalez/Vision-Robotics-Blog/main/assets/images/apriltag.png" 
	             alt="Detecci√≥n de AprilTags" 
	             style="width: 500px; height: auto; border-radius: 8px;">
	    </div>
	
	    <h3> Elecci√≥n de la baliza m√°s confiable</h3>
	    <p>
	        En cada imagen se pueden detectar m√∫ltiples balizas, pero no todas ofrecen la misma fiabilidad. 
	        Para seleccionar la mejor, se calcula el √°rea proyectada de cada baliza en la imagen, y se elige aquella con mayor √°rea aparente, ya que es la m√°s cercana y ofrece menor error en la estimaci√≥n de la pose.
	    </p>
	    <p>
	        Este criterio permite mejorar la precisi√≥n del sistema y evitar errores derivados de detecciones lejanas o parciales.
	    </p>
    </div>

    <!-- Contenido del segundo post -->
    <div id="post_Dia2_2" class="post-container" style="display: none;">
	    <h2>Estimaci√≥n de la pose del robot mediante transformaciones</h2>
	    
	    <h3> Transformaciones homog√©neas</h3>
	    <p>
	        A partir de la detecci√≥n de una baliza AprilTag, se puede calcular la posici√≥n y orientaci√≥n del robot respecto al mundo utilizando una serie de matrices de transformaci√≥n homog√©neas 4x4. 
	        El procedimiento se basa en transformar desde el sistema de referencia de la baliza hasta el del robot, pasando por la c√°mara.
	    </p>
	
	    <p>
	        La secuencia de transformaciones aplicada es la siguiente:
	    </p>
	
	    <pre>
	    T_robot_to_world = T_tag_to_world ¬∑ T_rot ¬∑ T_cam_to_tag ¬∑ T_rot_inv ¬∑ T_robot_to_cam
	    </pre>
	
	    <ul>
	        <li><strong>T_tag_to_world:</strong> Transformaci√≥n conocida desde el archivo YAML que define la posici√≥n de la baliza en el mundo.</li>
	        <li><strong>T_cam_to_tag:</strong> Se calcula <code>T_tag_to_cam</code> con <code>solvePnP</code> a partir de las esquinas detectadas y realizando la inversa obtenemos <code>T_cam_to_tag</code>.</li>
	        <li><strong>T_robot_to_cam:</strong> En el archivo .SDF, define la posici√≥n relativa de la c√°mara respecto al centro del robot. Se invierte para obtener el robot con respecto a la c√°mara. </li>
		<li><strong>T_rot:</strong> Define la rotaci√≥n de los ejes para pasar del sistema de referencia del simulador al marco √≥ptico (OpenCV). </li>
	    </ul>
	
	    <h3> Extracci√≥n de la pose del robot</h3>
	    <p>
	        Una vez calculada la matriz <code>T_robot_to_world</code>, se extrae la posici√≥n del robot de la √∫ltima columna de la matriz (x, y), y su orientaci√≥n (yaw) a partir de los elementos de rotaci√≥n con la f√≥rmula:
	    </p>
	
	    <pre>
	    pitch = np.arctan2(-R_f[2, 0], np.sqrt(R_f[0, 0]**2 + R_f[1, 0]**2))
            yaw = np.arctan2(R_f[1, 0] / np.cos(pitch), R_f[0, 0] / np.cos(pitch))
	    </pre>
	
	    <p>
	        Esto nos da la orientaci√≥n del robot en radianes respecto al eje Z del mundo, completando as√≠ su pose: (x, y, yaw).
	    </p>
	
	    <div style="text-align: center; margin-top: 20px;">
	        <img src="https://raw.githubusercontent.com/PaulaPenaGonzalez/Vision-Robotics-Blog/main/assets/images/localizacion.png" 
	             alt="Transformaciones para obtener la pose del robot" 
	             style="width: 520px; height: auto; border-radius: 8px;">
	    </div>
	
	    <h3> Consideraci√≥n del sistema de referencia √≥ptico</h3>
	    <p>
	        Como <code>solvePnP</code> devuelve la pose en el sistema de referencia √≥ptico de la c√°mara (donde X va a la derecha, Y hacia abajo y Z hacia delante), se aplica una conversi√≥n a sistema est√°ndar para que las transformaciones sean coherentes con la referencia del mundo y el robot. Esto se realiza mediante una rotaci√≥n compuesta:
	    </p>
	
	    <pre>
	    R_optical_to_standard = RotX(-90¬∞) ¬∑ RotZ(-90¬∞)
	    T_rot = np.eye(4)
            T_rot[:3, :3] = R_world_to_optical
	    </pre>
	
	    <p>
	        Esta rotaci√≥n se incorpora directamente y, de la misma manera, tambi√©n de forma inversa, en la multiplicaci√≥n final: ida y vuelta a coordenadas del marco √≥ptico (OpenCV).
	    </p>
    </div>


    <!-- Contenido del tercer post -->
    <div id="post_Dia3_3" class="post-container" style="display: none;">
	    <h2>Estimaci√≥n de la pose con odometr√≠a</h2>
	    
	    <h3> ¬øQu√© ocurre cuando no se detectan AprilTags?</h3>
	    <p>
	        En muchas ocasiones, el robot puede perder de vista las balizas. Para evitar quedarse sin estimaci√≥n de la pose, se emplea la <strong>odometr√≠a del robot</strong> como sistema de respaldo.
	    </p>
	    <p>
	        Dado que la odometr√≠a est√° sujeta a error acumulativo, no se usa como fuente primaria, sino que se integra solamente en los pasos en los que no se visualizan balizas. En cada iteraci√≥n, se calcula el cambio de posici√≥n y orientaci√≥n (delta) desde la √∫ltima lectura, y se aplica sobre la √∫ltima pose estimada por visi√≥n.
	    </p>
	
	    <p>
	        Se prob√≥ una alternativa basada en integrar velocidades lineales y angulares directamente:
	    </p>
	
	    <pre>
	    x += v * cos(theta) * dt
	    y += v * sin(theta) * dt
	    theta += w * dt
	    </pre>
	
	    <p>
	        Sin embargo, en la pr√°ctica, se observ√≥ que aprovechar directamente la odometr√≠a ofrec√≠a mejores resultados, ya que el simulador ya entrega valores acumulados de posici√≥n y orientaci√≥n.
	    </p>
	
	    <h3> Combinaci√≥n de fuentes</h3>
	    <p>
	        El sistema implementado combina ambas t√©cnicas:
	    </p>
	    <ul>
	        <li>Cuando se ve una baliza ‚Üí se usa la pose estimada por visi√≥n y se actualiza el estado.</li>
	        <li>Cuando no se ve ninguna baliza ‚Üí se actualiza la pose anterior con la diferencia de odometr√≠a.</li>
	    </ul>
	
	    <p>
	        Esta estrategia h√≠brida permite mantener una estimaci√≥n continua y bastante estable de la posici√≥n del robot en el entorno simulado.
	    </p>
    </div>

    <!-- Contenido del cuarto post -->
    <div id="post_Dia4_4" class="post-container" style="display: none;">
	    <h2>Selecci√≥n de la mejor baliza y visualizaci√≥n con colores</h2>
	
	    <h3> ¬øPor qu√© seleccionar una sola baliza?</h3>
	    <p>
	        Cuando varias AprilTags son visibles en la imagen, la estimaci√≥n de la pose puede variar entre unas y otras debido a la perspectiva o la distancia. Por ello, se decidi√≥ elegir √∫nicamente la m√°s <strong>confiable</strong> en cada iteraci√≥n, para mantener la consistencia.
	    </p>
	    <p>
	        La selecci√≥n se basa en el <strong>√°rea del contorno</strong> detectado: cuanto mayor es, m√°s cercana est√° la baliza y, por tanto, m√°s precisa ser√° su estimaci√≥n.
	    </p>
	
	    <h3> Visualizaci√≥n mejorada</h3>
	    <p>
	        Para ayudar en el desarrollo y depuraci√≥n del sistema, se implement√≥ una funci√≥n para pintar las balizas detectadas en la imagen:
	    </p>
	    <ul>
	        <li>Todas las balizas detectadas se pintan de <strong>verde</strong>.</li>
	        <li>La baliza seleccionada como la mejor se pinta de <strong>azul</strong>.</li>
	    </ul>
	
	    <p>
	        Esto permiti√≥ verificar visualmente qu√© baliza estaba siendo usada para estimar la pose, y ayud√≥ a ajustar correctamente el criterio de selecci√≥n.
	    </p>
	
	    <div style="text-align: center; margin-top: 20px;">
	        <img src="https://raw.githubusercontent.com/PaulaPenaGonzalez/Vision-Robotics-Blog/main/assets/images/balizas.png" 
	             alt="Selecci√≥n de la mejor baliza con resaltado azul" 
	             style="width: 500px; height: auto; border-radius: 8px;">
	    </div>
	
	    <p>
	        Esta estrategia garantiza que siempre se utilice la baliza m√°s cercana (y por tanto m√°s precisa), al tiempo que se ofrece una visualizaci√≥n clara del sistema en acci√≥n.
	    </p>
    </div>


    <!-- Contenido del quinto post -->
    <div id="post_Dia5_5" class="post-container" style="display: none;">
	    <h2> Movimiento del Robot y Estrategia de Exploraci√≥n </h2>
	
	    <p>
	        En esta etapa del proyecto, se integr√≥ el movimiento aut√≥nomo del robot para explorar el entorno en busca de balizas AprilTag. Esto fue fundamental para poder estimar su posici√≥n de forma continua, incluso cuando no se detectaban balizas durante algunos instantes.
	    </p>
	
	    <p>
	        El movimiento se control√≥ utilizando los comandos <code>HAL.setV()</code> para la velocidad lineal y <code>HAL.setW()</code> para la velocidad angular. De esta forma, el robot pudo avanzar en l√≠nea recta mientras realizaba un giro constante, permiti√©ndole recorrer el espacio de forma helicoidal.
	    </p>
	
	    <p>
	        Adem√°s, para evitar que el robot quedase atascado o siguiera un patr√≥n demasiado repetitivo, se incorpor√≥ un peque√±o giro aleatorio usando la librer√≠a <code>random</code>. Esto hac√≠a que el robot variara ligeramente su direcci√≥n de forma impredecible, maximizando as√≠ la probabilidad de detectar nuevas balizas.
	    </p>
	
	    <p>
	        Este comportamiento activo es especialmente √∫til en entornos cerrados o con obst√°culos, donde las balizas pueden estar parcialmente ocultas o fuera del campo de visi√≥n en momentos puntuales.
	    </p>
	
    </div>

    <!-- Contenido del sexto post -->
    <div id="post_Dia6_6" class="post-container" style="display: none;">
	
	    <h2>Problemas encontrados y ajustes</h2>
	
	    <ul>
	        <li>
	            <strong>Desajuste entre sistemas de coordenadas:</strong> OpenCV utiliza un marco √≥ptico (X: derecha, Y: abajo, Z: hacia adelante), mientras que el simulador usa un marco est√°ndar (X: adelante, Y: izquierda, Z: arriba). Se aplicaron rotaciones de -90¬∫ en Z y X para alinear ambos sistemas.
	        </li>
	        <li>
	            <strong>Estimaci√≥n incorrecta del yaw:</strong> Inicialmente, el c√°lculo con <code>arctan2</code> devolv√≠a resultados inconsistentes debido a los ejes mal alineados. Tras corregir los marcos de referencia, el √°ngulo de orientaci√≥n se estim√≥ correctamente.
	        </li>
	        <li>
	            <strong>Falsos positivos o detecci√≥n m√∫ltiple de balizas:</strong> En ocasiones se detectaban varias balizas simult√°neamente. Se opt√≥ por seleccionar la m√°s cercana (mayor √°rea proyectada) por ser la que proporciona menor error en <code>solvePnP</code>.
	        </li>
	    </ul>
	
    </div>

    <!-- Contenido del s√©ptimo post -->
    <div id="post_Dia7_7" class="post-container" style="display: none;">
	    <h2> Conclusi√≥n y Resultados Finales </h2>
	
	    <p>
	        Al finalizar el desarrollo del sistema de localizaci√≥n visual con AprilTags, se ha logrado un sistema robusto capaz de estimar la posici√≥n del robot en el entorno simulador, combinando informaci√≥n visual y odom√©trica.
	    </p>
	
	    <p>
	        El sistema funciona de la siguiente forma:
	        <ul>
	            <li> Cuando se detecta una baliza AprilTag, se calcula la pose del robot mediante una cadena de transformaciones desde el marco de la baliza hasta el del robot.</li>
	            <li> Cuando no hay detecci√≥n, se emplea la odometr√≠a del simulador para mantener una estimaci√≥n aproximada de la posici√≥n.</li>
	        </ul>
	    </p>
	
	    <h3>Valoraci√≥n del rendimiento</h3>
	    <ul>
	        <li><strong>Precisi√≥n visual:</strong> Alta cuando hay detecci√≥n.</li>
	        <li><strong>Odomet√≠a:</strong> Introduce ruido progresivamente, pero √∫til a corto plazo.</li>
	        <li><strong>Recuperaci√≥n:</strong> Muy buena al volver a detectar una baliza.</li>
	        <li><strong>Generalizaci√≥n:</strong> Compatible con simuladores y entornos reales con balizas conocidas.</li>
	    </ul>

	    <h3>Posibles mejoras</h3>
	    <p>
	        Existe un campo de mejora en el c√°lculo de odometr√≠a para reducir el error acumulado cuando no se encuentran balizas.
	    </p>
	
	    <h3>V√≠deo demostrativo</h3>
	    <div style="text-align: center; margin: 30px 0;">
	        <iframe width="560" height="315" src="https://www.youtube.com/embed/iPVz_WGa9l0" frameborder="0" allowfullscreen></iframe>
	    </div>
	
	    <p>
	        En el v√≠deo se puede ver c√≥mo el robot inicia con detecci√≥n visual, pierde contacto con las balizas (pasando a odometr√≠a) y finalmente recupera su posici√≥n gracias a una nueva detecci√≥n.
	    </p>
	
    </div>


    <script>
    function mostrarPost(categoria, num) {
        let categorias = ["holonomico", "ackerman"];
        
        // Ocultar todos los posts de ambas categor√≠as
        categorias.forEach(cat => {
            for (let i = 1; i <= 9; i++) {
                let post = document.getElementById(`post_${cat}_${i}`);
                if (post) {
                    post.style.opacity = 0;
                    post.style.display = 'none';
                }
            }
        });

        // Mostrar el post seleccionado
        setTimeout(() => {
            let post = document.getElementById(`post_${categoria}_${num}`);
            if (post) {
                post.style.display = 'block';
                setTimeout(() => {
                    post.style.opacity = 1;
                    post.style.transform = 'translateY(0)';
                }, 50);
            }
        }, 200);
    }
</script>

    <!-- üìå Footer al final de la p√°gina -->
    <footer>
        <p><a href="../index.html">Volver a la p√°gina principal</a></p>
    </footer>
</body>
</html>
