<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Localización del robot con AprilTags</title>
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <h1> Localización del robot utilizando AprilTags </h1>
        <p>Estimación de la pose del robot a partir de visión artificial y odometría.</p>
    </header>

    <!-- Introducción antes de las publicaciones -->
    <div class="container">
        <p>
        En este proyecto hemos implementado un sistema de localización de un robot móvil mediante el uso de balizas AprilTags y técnicas de visión por computador.
        </p>
        <p>
        A partir de las imágenes capturadas por la cámara del robot, se detectan las balizas y se calcula la posición y orientación del robot con respecto al mundo aplicando transformaciones geométricas (roto-traslaciones) entre diferentes sistemas de referencia: cámara, baliza, robot y mundo.
        </p>
        <p>
        Cuando el robot no es capaz de ver ninguna baliza, el sistema recurre a la odometría para estimar su nueva posición, sumando los pequeños desplazamientos detectados respecto a la última posición conocida.
        </p>
        <p>
        Este enfoque híbrido permite mantener una localización continua y razonablemente precisa incluso en entornos parcialmente observables.
        </p>

        <!-- Imagen destacada del proyecto -->
        <div style="text-align: center; margin-top: 20px;">
            <img src="https://raw.githubusercontent.com/PaulaPenaGonzalez/Vision-Robotics-Blog/main/assets/images/proyecto3.png" 
                 alt="Localización del robot con AprilTags" 
                 style="width: 600px; height: auto; border-radius: 8px;">
        </div>
    </div>


    <div class="container">
        <h2>Publicaciones - Reconstrucción 3D</h2>
        <button onclick="mostrarPost('Dia1', 1)"> Primer post </button>
        <button onclick="mostrarPost('Dia2', 2)"> Segundo post</button>
        <button onclick="mostrarPost('Dia3', 3)"> Tercer post</button>
        <button onclick="mostrarPost('Dia4', 4)"> Cuarto post</button>
	<button onclick="mostrarPost('Dia5', 5)"> Quinto post</button>
	<button onclick="mostrarPost('Dia6', 6)"> Sexto post</button>
	<button onclick="mostrarPost('Dia7', 7)"> Séptimo post</button>
    </div>



    <!-- Contenido del primer post -->
    <div id="post_Dia1_1" class="post-container" style="display: none;">
	    <h2>Objetivo del proyecto y detección de balizas AprilTags</h2>
	    
	    <h3> Objetivo principal</h3>
	    <p>
	        El objetivo de este proyecto es implementar un sistema de localización autónoma de un robot móvil en un entorno cerrado utilizando balizas AprilTags como referencias visuales. 
	    </p>
	    <p>
	        Para lograrlo, se emplea una cámara montada sobre el robot y se calcula su posición y orientación (pose) a partir de la detección y análisis de las balizas. 
	        Cuando las balizas no están visibles, se recurre a los datos de odometría para mantener una estimación continua.
	    </p>
	    
	    <h3> Detección de AprilTags</h3>
	    <p>
	        Se utiliza la biblioteca <code>pyapriltags</code> para detectar las balizas en las imágenes capturadas por la cámara del robot. Esta herramienta proporciona las coordenadas de las esquinas de cada baliza y su identificador único.
	    </p>
	    <p>
	        A partir de estas esquinas, se aplica el algoritmo <code>solvePnP</code> de OpenCV para estimar la pose relativa de la caámara con respecto a la baliza. Esta información, combinada con la pose conocida de la baliza en el mundo, permite calcular la pose del robot mediante una serie de roto-traslaciones.
	    </p>
	    
	    <div style="text-align: center; margin-top: 20px;">
	        <img src="https://raw.githubusercontent.com/PaulaPenaGonzalez/Vision-Robotics-Blog/main/assets/images/apriltags.png" 
	             alt="Detección de AprilTags" 
	             style="width: 500px; height: auto; border-radius: 8px;">
	    </div>
	
	    <h3> Elección de la baliza más confiable</h3>
	    <p>
	        En cada imagen se pueden detectar múltiples balizas, pero no todas ofrecen la misma fiabilidad. 
	        Para seleccionar la mejor, se calcula el área proyectada de cada baliza en la imagen, y se elige aquella con mayor área aparente, ya que es la más cercana y ofrece menor error en la estimación de la pose.
	    </p>
	    <p>
	        Este criterio permite mejorar la precisión del sistema y evitar errores derivados de detecciones lejanas o parciales.
	    </p>
    </div>

    <!-- Contenido del segundo post -->
    <div id="post_Dia2_2" class="post-container" style="display: none;">
	    <h2>Estimación de la pose del robot mediante transformaciones</h2>
	    
	    <h3> Transformaciones homogéneas</h3>
	    <p>
	        A partir de la detección de una baliza AprilTag, se puede calcular la posición y orientación del robot respecto al mundo utilizando una serie de matrices de transformación homogéneas 4x4. 
	        El procedimiento se basa en transformar desde el sistema de referencia de la baliza hasta el del robot, pasando por la cámara.
	    </p>
	
	    <p>
	        La secuencia de transformaciones aplicada es la siguiente:
	    </p>
	
	    <pre>
	    T_robot_to_world = T_tag_to_world · T_rot · T_cam_to_tag · T_rot_inv · T_robot_to_cam
	    </pre>
	
	    <ul>
	        <li><strong>T_tag_to_world:</strong> Transformación conocida desde el archivo YAML que define la posición de la baliza en el mundo.</li>
	        <li><strong>T_cam_to_tag:</strong> Se calcula <code>T_tag_to_cam</code> con <code>solvePnP</code> a partir de las esquinas detectadas y realizando la inversa obtenemos <code>T_cam_to_tag</code>.</li>
	        <li><strong>T_robot_to_cam:</strong> En el archivo .SDF, define la posición relativa de la cámara respecto al centro del robot. Se invierte para obtener el robot con respecto a la cámara. </li>
		<li><strong>T_rot:</strong> Define la rotación de los ejes para pasar del sistema de referencia del simulador al marco óptico (Open CV). </li>
	    </ul>
	
	    <h3> Extracción de la pose del robot</h3>
	    <p>
	        Una vez calculada la matriz <code>T_robot_to_world</code>, se extrae la posición del robot de la última columna de la matriz (x, y), y su orientación (yaw) a partir de los elementos de rotación con la fórmula:
	    </p>
	
	    <pre>
	    pitch = np.arctan2(-R_f[2, 0], np.sqrt(R_f[0, 0]**2 + R_f[1, 0]**2))
            yaw = np.arctan2(R_f[1, 0] / np.cos(pitch), R_f[0, 0] / np.cos(pitch))
	    </pre>
	
	    <p>
	        Esto nos da la orientación del robot en radianes respecto al eje Z del mundo, completando así su pose: (x, y, yaw).
	    </p>
	
	    <div style="text-align: center; margin-top: 20px;">
	        <img src="https://raw.githubusercontent.com/TU_USUARIO/TU_REPO/main/assets/images/transformaciones_robot_pose.png" 
	             alt="Transformaciones para obtener la pose del robot" 
	             style="width: 520px; height: auto; border-radius: 8px;">
	    </div>
	
	    <h3> Consideración del sistema de referencia óptico</h3>
	    <p>
	        Como <code>solvePnP</code> devuelve la pose en el sistema de referencia óptico de la cámara (donde X va a la derecha, Y hacia abajo y Z hacia delante), se aplica una conversión a sistema estándar para que las transformaciones sean coherentes con la referencia del mundo y el robot. Esto se realiza mediante una rotación compuesta:
	    </p>
	
	    <pre>
	    R_optical_to_standard = RotX(-90°) · RotZ(-90°)
	    T_rot = np.eye(4)
            T_rot[:3, :3] = R_world_to_optical
	    </pre>
	
	    <p>
	        Esta rotación se incorpora directamente y, de la misma manera, también de forma inversa, en la multiplicación final: idea y vuelta a coordenadas del marco óptico (Open CV).
	    </p>
    </div>


    <!-- Contenido del tercer post -->
    <div id="post_Dia3_3" class="post-container" style="display: none;">
	<h2> Detección y selección de puntos característicos </h2>
        
	<p>Uno de los pilares de la reconstrucción 3D a partir de visión estéreo es la <strong>geometría epipolar</strong>. En este post abordamos cómo obtener la línea epipolar en la imagen derecha a partir de un punto característico detectado en la imagen izquierda.</p>

	<div style="text-align: center; margin-top: 20px;">
	        <img src="https://raw.githubusercontent.com/PaulaPenaGonzalez/Vision-Robotics-Blog/main/assets/images/TRIANGULACION.png" 
	             alt="Reconstrucción 3D a partir de bordes" 
	             style="width: 350px; height: auto; border-radius: 8px;">
    	</div>
	
	<h3> Retroproyección del punto izquierdo </h3>

	<p>Cada punto característico extraído de la imagen izquierda es transformado a coordenadas del mundo real mediante retroproyección. Para ello, se utiliza la matriz de proyección inversa de la cámara izquierda, proporcionada por la herramienta <code>HAL.backproject</code>.</p>
        
        <h3>Vector director del rayo</h3>
        <p>Con el punto 3D obtenido y la posición de la cámara izquierda, se calcula el vector director del rayo de retroproyección:</p>
    	<p><code>u = punto_3d - centro_cámara</code></p>
    	<p>Este vector se normaliza para definir una dirección en el espacio desde el centro de la cámara.</p>
        
        <h3>Proyección del rayo en la imagen derecha</h3>

	<p>Se seleccionan dos puntos arbitrarios sobre el rayo en 3D (por ejemplo, a cierta distancia a lo largo del vector director) y se proyectan sobre la imagen derecha utilizando la función <code>HAL.project</code>.</p>
	<p>Posteriormente, se convierten estas coordenadas proyectadas de espacio óptico a espacio gráfico con <code>HAL.opticalToGrafic</code>.</p>
	
	<h3> Cálculo de la línea epipolar</h3>
	<p>Una vez tenemos los dos puntos proyectados en la imagen derecha, la línea epipolar se calcula como la recta que pasa por ambos:</p>
	<p><code>line = np.cross(p1_2d, p2_2d)</code></p>
	<p>Esto nos da los coeficientes <code>(a, b, c)</code> de la recta epipolar en la forma <code>ax + by + c = 0</code>, que se dibujará sobre la imagen derecha para restringir la búsqueda de homólogos a esa línea.</p>

	<div style="text-align: center; margin-top: 20px;">
	        <img src="https://raw.githubusercontent.com/PaulaPenaGonzalez/Vision-Robotics-Blog/main/assets/images/RectaEpipolar.jpg" 
	             alt="Reconstrucción 3D a partir de bordes" 
	             style="width: 300px; height: auto; border-radius: 8px;">
    	</div>
	
    </div>

    <!-- Contenido del cuarto post -->
    <div id="post_Dia4_4" class="post-container" style="display: none;">
	
	<h2>Emparejamiento por correlación (búsqueda de homólogos)</h2>

	<p>Una vez definida la línea epipolar en la imagen derecha, el siguiente paso es buscar el punto homólogo correspondiente al punto de la imagen izquierda. Esta tarea se realiza mediante correlación de ventanas y técnicas de comparación basadas en diferencias de intensidad.</p>
	
        <h3>Función <code>buscar_match_epipolar_Canny()</code></h3>

	<p>Se diseñó una función que recorre la línea epipolar en la imagen derecha evaluando múltiples coordenadas posibles, comparando cada una con la región del punto original en la imagen izquierda.</p>

        <ul>
	        <li>Para cada valor de <code>x</code> a lo largo de la línea epipolar, se calcula el correspondiente <code>y</code> usando la ecuación de la recta <code>y = -(a·x + c)/b</code>.</li>
	        <li>Se extrae una ventana centrada en <code>(x, y)</code> en la imagen derecha y se compara con una ventana de mismo tamaño centrada en el punto original de la imagen izquierda.</li>
	        <li>La comparación se realiza usando el error <strong>SSD normalizado</strong> (Sum of Squared Differences), lo cual penaliza las diferencias de intensidad.</li>
    	</ul> 

	<p>Una vez recorrida toda la línea epipolar, se selecciona el punto con menor SSD como mejor candidato, siempre que esté por debajo de un umbral que garantiza una buena correspondencia.</p>

	<h3>Filtro adicional por color</h3>
    	<p>Para mejorar aún más la fiabilidad de los matches, se quiso implementar un filtro adicional que compara el color del píxel en la imagen izquierda con el color del píxel candidato en la derecha.</p>

	<ul>
		<li>Se calcula la distancia euclídea entre ambos colores (en espacio RGB).</li>
	        <li>Si la diferencia es demasiado alta (por encima de un umbral establecido), se descarta el match aunque tenga un buen SSD.</li>
	</ul>

	<p>Este enfoque combinado de <strong>correlación espacial</strong> y <strong>consistencia de color</strong> hubiese podido mejorar significativamente la robustez del emparejamiento, reduciendo falsos positivos y contribuyendo a una nube de puntos 3D más precisa. Sin embargo en el modelo final, solo se implementó la correlación espacial.</p>

	<p>A continuación se muestra un video del proceso de correlación en ambas imágenes:</p>

	

    </div>

    <!-- Contenido del quinto post -->
    <div id="post_Dia5_5" class="post-container" style="display: none;">
	
	<h2>Triangulación del punto 3D</h2>
	    
	<p>Una vez identificada una pareja de píxeles correspondientes (uno en cada imagen), el siguiente paso es estimar la posición 3D del punto de la escena que proyecta en ambas cámaras. Para ello, utilizamos una técnica basada en la geometría de retroproyección.</p>

	<h3>Rayos de retroproyección</h3>
  	<p>Cada píxel, al ser retroproyectado mediante la matriz de calibración y la posición de la cámara, genera un rayo que apunta desde el centro óptico hacia la escena. Ambos rayos (izquierdo y derecho) deberían, idealmente, cruzarse en el espacio tridimensional.</p>

	<h3>Implementación de <code>triangulate_midpoint()</code></h3>
	<ul>
	    <li>Se planteó y resolvió un sistema lineal para encontrar los puntos más cercanos entre ambos rayos.</li>
	    <li>Estos puntos se denominan <code>P1</code> y <code>P2</code>, uno sobre cada rayo.</li>
	    <li>El punto 3D estimado es el punto medio entre ambos: <strong>el centro del segmento mínimo entre rayos</strong>.</li>
	</ul>

	<div style="text-align: center; margin: 30px 0;">
	<iframe width="560" height="315" src="https://www.youtube.com/embed/Fd0WqJa0TB8" frameborder="0" allowfullscreen></iframe>
	</div>

	<h3>Filtrado de puntos no fiables (outliers)</h3>
	<ul>
	    <li>Si la distancia entre los rayos (<code>‖P1 - P2‖</code>) es demasiado alta, el punto se descarta por inconsistencia geométrica.</li>
	    <li>También se descartan las soluciones en las que los parámetros del sistema lineal (<code>t</code> o <code>s</code>) resultan negativos. Esto indica que el punto estaría "detrás" de alguna de las cámaras, lo cual no tiene sentido físico.</li>
	</ul>

	<p>Con esta función, se garantiza que cada punto de la nube reconstruida tiene una base sólida desde el punto de vista geométrico y espacial. Solo se conservan aquellos puntos que están bien triangulados y situados en la región visual válida de ambas cámaras.</p>
	
    </div>

    <!-- Contenido del sexto post -->
    <div id="post_Dia6_6" class="post-container" style="display: none;">
        <h2> Proyección y visualización </h2>
	
        <p>Tras obtener la posición 3D de un punto mediante triangulación, el siguiente paso fue representarlo de forma visual dentro del entorno. Para ello, se empleó el visor 3D de Unibotics como herramienta de visualización.</p>

	<h3>Conversión de unidades</h3>
  	<p>Como las posiciones reconstruidas están en milímetros (debido a la escala de las cámaras), fue necesario dividir por 100 para convertirlas a decímetros. Esto permite una visualización adecuada y proporcional dentro del entorno virtual.</p>

	<h3>Color de los puntos</h3>
	<p>Cada punto 3D se visualiza no solo por su posición, sino también por el color extraído del píxel izquierdo original. Este detalle aporta riqueza visual a la nube de puntos, ya que cada uno conserva la tonalidad del objeto real que lo originó.</p>
	<ul>
	    <li>Se accede al color del píxel en la imagen izquierda (usualmente en formato BGR).</li>
	    <li>Se convierte a RGB para usarlo correctamente en el visor.</li>
	    <li>Se construye una tupla con la forma <code>(x, y, z, r, g, b)</code>.</li>
	</ul>

	<h3>Visualización en el entorno 3D</h3>
  	<p>Con la posición ajustada y el color asociado, cada punto se envía al visor con la función <code>GUI.ShowNewPoints()</code>. Así, la nube de puntos 3D se va formando de forma interactiva, punto a punto, representando fielmente la geometría y el aspecto de la escena original.</p>

  	<p>Este paso permite validar visualmente la reconstrucción y ajustar parámetros si se detectan errores en la escala, el color o la alineación de los puntos generados.</p>

    </div>

    <!-- Contenido del séptimo post -->
    <div id="post_Dia7_7" class="post-container" style="display: none;">
        <h2> Limpieza y refinamiento </h2>
        <p>Una vez generada la nube de puntos 3D, el siguiente paso fundamental fue su depuración. La reconstrucción puede incluir puntos erróneos u outliers debido a emparejamientos incorrectos, ruido o geometrías mal trianguladas.</p>

	<h3>Filtrado por distancia entre rayos</h3>
 	<p>Durante la triangulación, se calculó la distancia entre los rayos de retroproyección de cada cámara. Si esta distancia era demasiado grande, se consideraba que los rayos no se cruzaban de forma fiable, y el punto 3D correspondiente se descartaba.</p>

	<h3>Umbral sobre los parámetros de triangulación <code>t</code> y <code>s</code></h3>
	<p> Otro criterio de validación consistió en comprobar que los parámetros <code>t</code> y <code>s</code> obtenidos durante la resolución del sistema de triangulación fueran positivos y estuvieran dentro de un rango aceptable.</p>
	<p>Estos valores determinan la posición del punto a lo largo de los rayos, por lo que si son negativos o extremadamente grandes, significa que el punto reconstruido estaría fuera del campo visual de la escena. </p>
	
	<p> Gracias a la incorporación de estos dos filtros, la nube de puntos final presenta una geometría mucho más coherente y limpia,facilitando su análisis visual y reduciendo significativamente los outliers.</p>

	<p>A cotinuación se muestra un video con el resultado final de la reconstrucción 3D:</p>
	
	<div style="text-align: center; margin: 30px 0;">
	<iframe width="560" height="315" src="https://www.youtube.com/embed/ceEEqg4W2bo" frameborder="0" allowfullscreen></iframe>
	</div>
    
    </div>

    <script>
    function mostrarPost(categoria, num) {
        let categorias = ["holonomico", "ackerman"];
        
        // Ocultar todos los posts de ambas categorías
        categorias.forEach(cat => {
            for (let i = 1; i <= 9; i++) {
                let post = document.getElementById(`post_${cat}_${i}`);
                if (post) {
                    post.style.opacity = 0;
                    post.style.display = 'none';
                }
            }
        });

        // Mostrar el post seleccionado
        setTimeout(() => {
            let post = document.getElementById(`post_${categoria}_${num}`);
            if (post) {
                post.style.display = 'block';
                setTimeout(() => {
                    post.style.opacity = 1;
                    post.style.transform = 'translateY(0)';
                }, 50);
            }
        }, 200);
    }
</script>

    <!-- 📌 Footer al final de la página -->
    <footer>
        <p><a href="../index.html">Volver a la página principal</a></p>
    </footer>
</body>
</html>
