<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Proyecto 1 - Detección de Bordes</title>
    <link rel="stylesheet" href="../styles.css">
</head>
<body>
    <header>
        <h1> Reconstrucción 3D de una escena 2D </h1>
        <p> Progreso y avances de este proyecto. </p>
    </header>

    <!-- Introducción antes de las publicaciones -->
    <div class="container">
    	<p>
        En este proyecto, se ha desarrollado un sistema de reconstrucción 3D a partir de imágenes estéreo. A través de técnicas clásicas de visión por computador, como la geometría epipolar, la correlación de ventanas y la triangulación de rayos, se genera una nube de puntos coloreada que representa la escena capturada por las cámaras. 
    	</p>
    	<p>
        A continuación, puedes leer las publicaciones relacionadas con este proyecto, donde se detallan los avances, problemas encontrados y soluciones aplicadas. 
	</p>

	<!-- Imagen de la reconstrucción -->
    	<div style="text-align: center; margin-top: 20px;">
	        <img src="https://raw.githubusercontent.com/PaulaPenaGonzalez/Vision-Robotics-Blog/main/assets/images/proyecto2.jpg" 
	             alt="Reconstrucción 3D a partir de bordes" 
	             style="width: 600px; height: auto; border-radius: 8px;">
    	</div>
    </div>

    <div class="container">
        <h2>Publicaciones - Reconstrucción 3D</h2>
        <button onclick="mostrarPost('Dia1', 1)"> Primer post </button>
        <button onclick="mostrarPost('Dia2', 2)"> Segundo post</button>
        <button onclick="mostrarPost('Dia3', 3)"> Tercer post</button>
        <button onclick="mostrarPost('Dia4', 4)"> Cuarto post</button>
	<button onclick="mostrarPost('Dia5', 5)"> Quinto post</button>
	<button onclick="mostrarPost('Dia6', 6)"> Sexto post</button>
	<button onclick="mostrarPost('Dia7', 6)"> Séptimo post</button>
    </div>

    <!-- Contenido del primer post -->
    <div id="post_Dia1_1" class="post-container" style="display: none;">
	<h2> Preparación inicial y lectura de imágenes </h2>
	<h3>Objetivo:</h3>
    	<p> Cargar y preprocesar las imágenes estéreo capturadas por las dos cámaras calibradas (izquierda y derecha). Fundamental para extraer la información visual relevante que servirá como base para todo el sistema de reconstrucción 3D.</p>

    	<h3> Captura de Imágenes</h3>

    	<p>Las imágenes izquierda y derecha fueron obtenidas desde el simulador Unibotics. Estas imágenes corresponden a la misma escena vista desde dos ángulos ligeramente diferentes, lo que permite generar percepción de profundidad gracias al paralaje.</p>

	<div style="text-align: center; margin-top: 20px;">
	        <img src="https://raw.githubusercontent.com/PaulaPenaGonzalez/Vision-Robotics-Blog/main/assets/images/ImagenLImagenR.jpg" 
	             alt="Reconstrucción 3D a partir de bordes" 
	             style="width: 300px; height: auto; border-radius: 8px;">
    	</div>
	    
    	<h3> Conversión a Escala de Grises</h3>
    	<p> Para simplificar el análisis visual y reducir la carga computacional, se realizó una conversión de las imágenes a escala de grises. Esto permite centrarse en la estructura de la escena sin que el color interfiera en las operaciones iniciales como la detección de bordes.</p>

    	<h3>Detección de Bordes</h3>
	<p>La etapa clave en este punto fue aplicar un detector de bordes, específicamente el método de Canny. Esta técnica resalta los contornos y líneas más relevantes de la escena, permitiendo identificar los puntos característicos que luego se utilizarán para hallar correspondencias entre ambas imágenes. Se hizo un ajuste manual de los parámetros del detector para encontrar un equilibrio entre sensibilidad (captar todos los bordes importantes) y robustez (evitar bordes producidos por ruido o detalles irrelevantes).</p>

	<div style="text-align: center; margin-top: 20px;">
	        <img src="https://raw.githubusercontent.com/PaulaPenaGonzalez/Vision-Robotics-Blog/main/assets/images/Bordes.jpg" 
	             alt="Reconstrucción 3D a partir de bordes" 
	             style="width: 300px; height: auto; border-radius: 8px;">
    	</div>

    	<h3>Visualización y Verificación</h3>
    	<p>Una vez obtenidas las máscaras de bordes, se visualizaron ambas imágenes con sus respectivos contornos resaltados. Esto ayudó a comprobar que los parámetros elegidos proporcionaban una buena base para continuar con la reconstrucción.</p>
    
    </div>

    <!-- Contenido del tercer post -->
    <div id="post_Dia3_3" class="post-container" style="display: none;">
	<h2> Detección y selección de puntos característicos </h2>
        
	<p>Uno de los pilares de la reconstrucción 3D a partir de visión estéreo es la <strong>geometría epipolar</strong>. En este post abordamos cómo obtener la línea epipolar en la imagen derecha a partir de un punto característico detectado en la imagen izquierda.</p>
        
	<h3> Retroproyección del punto izquierdo </h3>

	<p>Cada punto característico extraído de la imagen izquierda es transformado a coordenadas del mundo real mediante retroproyección. Para ello, se utiliza la matriz de proyección inversa de la cámara izquierda, proporcionada por la herramienta <code>HAL.backproject</code>.</p>
        
        <h3>Vector director del rayo</h3>
        <p>Con el punto 3D obtenido y la posición de la cámara izquierda, se calcula el vector director del rayo de retroproyección:</p>
    	<p><code>u = punto_3d - centro_cámara</code></p>
    	<p>Este vector se normaliza para definir una dirección en el espacio desde el centro de la cámara.</p>
        
        <h3>Proyección del rayo en la imagen derecha</h3>

	<p>Se seleccionan dos puntos arbitrarios sobre el rayo en 3D (por ejemplo, a cierta distancia a lo largo del vector director) y se proyectan sobre la imagen derecha utilizando la función <code>HAL.project</code>.</p>
	<p>Posteriormente, se convierten estas coordenadas proyectadas de espacio óptico a espacio gráfico con <code>HAL.opticalToGrafic</code>.</p>
	
	<h3> Cálculo de la línea epipolar</h3>
	<p>Una vez tenemos los dos puntos proyectados en la imagen derecha, la línea epipolar se calcula como la recta que pasa por ambos:</p>
	<p><code>line = np.cross(p1_2d, p2_2d)</code></p>
	<p>Esto nos da los coeficientes <code>(a, b, c)</code> de la recta epipolar en la forma <code>ax + by + c = 0</code>, que se dibujará sobre la imagen derecha para restringir la búsqueda de homólogos a esa línea.</p>

	<div style="text-align: center; margin-top: 20px;">
	        <img src="https://raw.githubusercontent.com/PaulaPenaGonzalez/Vision-Robotics-Blog/main/assets/images/RectaEpipolar.jpg" 
	             alt="Reconstrucción 3D a partir de bordes" 
	             style="width: 300px; height: auto; border-radius: 8px;">
    	</div>
	
    </div>

    <!--Contenido del segundo post -->
    <div id="post_Dia2_2" class="post-container" style="display: none;">
        <h2> ¿Qué significa detectar puntos característicos? </h2>
        <p>En el proceso de reconstrucción 3D, identificar los puntos más relevantes de una imagen es esencial. No todos los píxeles aportan información útil: necesitamos centrarnos en aquellos que definen contornos, estructuras y geometrías bien marcadas en la escena. En este post, abordamos cómo se extraen estos puntos a partir de una imagen ya procesada por un detector de bordes.</p>
        
        <h3> Selección a partir de bordes detectados </h3>
	<p>Una vez obtenida la máscara de bordes mediante el detector de Canny, lo que tenemos es una imagen en blanco y negro donde los bordes aparecen marcados en blanco (valor alto).</p>
	<p>A partir de aquí, el objetivo es localizar las coordenadas de estos píxeles blancos que representan los contornos más significativos.</p>
	
	<p>Para ello, se recorre la imagen buscando todas las posiciones en las que el valor del píxel supere cierto umbral (en este caso, simplemente donde el valor es mayor que cero). </p>
	<p>Esto nos devuelve las coordenadas (x, y) de los bordes relevantes. Así se construye una lista de puntos que actuarán como candidatos para buscar correspondencias entre imágenes izquierda y derecha.</p>
        
        <h3> Un pequeño extra </h3>
        <p>Durante las pruebas, se decidió no procesar todos los puntos a la vez, sino seleccionar uno de cada pocos (por ejemplo, cada tres puntos). </p>
	<p>Esto ayuda a mantener el rendimiento y obtener una nube de puntos más limpia y distribuida de forma uniforme. Con esta lista de puntos característica ya lista, estamos preparados para pasar al siguiente paso: proyectar cada uno de ellos y buscar su homólogo en la segunda imagen </p>
        
    </div>

    <!-- Contenido del cuarto post -->
    <div id="post_Dia4_4" class="post-container" style="display: none;">
	
	<h2>Emparejamiento por correlación (búsqueda de homólogos)</h2>

	<p>Una vez definida la línea epipolar en la imagen derecha, el siguiente paso es buscar el punto homólogo correspondiente al punto de la imagen izquierda. Esta tarea se realiza mediante correlación de ventanas y técnicas de comparación basadas en diferencias de intensidad.</p>
	
        <h3>Función <code>buscar_match_epipolar_Canny()</code></h3>

	<p>Se diseñó una función que recorre la línea epipolar en la imagen derecha evaluando múltiples coordenadas posibles, comparando cada una con la región del punto original en la imagen izquierda.</p>

        <ul>
	        <li>Para cada valor de <code>x</code> a lo largo de la línea epipolar, se calcula el correspondiente <code>y</code> usando la ecuación de la recta <code>y = -(a·x + c)/b</code>.</li>
	        <li>Se extrae una ventana centrada en <code>(x, y)</code> en la imagen derecha y se compara con una ventana de mismo tamaño centrada en el punto original de la imagen izquierda.</li>
	        <li>La comparación se realiza usando el error <strong>SSD normalizado</strong> (Sum of Squared Differences), lo cual penaliza las diferencias de intensidad.</li>
    	</ul> 

	<p>Una vez recorrida toda la línea epipolar, se selecciona el punto con menor SSD como mejor candidato, siempre que esté por debajo de un umbral que garantiza una buena correspondencia.</p>

	<h3>Filtro adicional por color</h3>
    	<p>Para mejorar aún más la fiabilidad de los matches, se implementó un filtro adicional que compara el color del píxel en la imagen izquierda con el color del píxel candidato en la derecha.</p>

	<ul>
		<li>Se calcula la distancia euclídea entre ambos colores (en espacio RGB).</li>
	        <li>Si la diferencia es demasiado alta (por encima de un umbral establecido), se descarta el match aunque tenga un buen SSD.</li>
	</ul>

	<p>Este enfoque combinado de <strong>correlación espacial</strong> y <strong>consistencia de color</strong> mejora significativamente la robustez del emparejamiento, reduciendo falsos positivos y contribuyendo a una nube de puntos 3D más precisa.</p>

    </div>

    <!-- Contenido del quinto post -->
    <div id="post_Dia5_5" class="post-container" style="display: none;">
	
	<h2>Triangulación del punto 3D</h2>
	    
	<p>Una vez identificada una pareja de píxeles correspondientes (uno en cada imagen), el siguiente paso es estimar la posición 3D del punto de la escena que proyecta en ambas cámaras. Para ello, utilizamos una técnica basada en la geometría de retroproyección.</p>

	<h3>Rayos de retroproyección</h3>
  	<p>Cada píxel, al ser retroproyectado mediante la matriz de calibración y la posición de la cámara, genera un rayo que apunta desde el centro óptico hacia la escena. Ambos rayos (izquierdo y derecho) deberían, idealmente, cruzarse en el espacio tridimensional.</p>

	<h3>Implementación de <code>triangulate_midpoint()</code></h3>
	<ul>
	    <li>Se planteó y resolvió un sistema lineal para encontrar los puntos más cercanos entre ambos rayos.</li>
	    <li>Estos puntos se denominan <code>P1</code> y <code>P2</code>, uno sobre cada rayo.</li>
	    <li>El punto 3D estimado es el punto medio entre ambos: <strong>el centro del segmento mínimo entre rayos</strong>.</li>
	</ul>

	<h3>Filtrado de puntos no fiables (outliers)</h3>
	<ul>
	    <li>Si la distancia entre los rayos (<code>‖P1 - P2‖</code>) es demasiado alta, el punto se descarta por inconsistencia geométrica.</li>
	    <li>También se descartan las soluciones en las que los parámetros del sistema lineal (<code>t</code> o <code>s</code>) resultan negativos. Esto indica que el punto estaría "detrás" de alguna de las cámaras, lo cual no tiene sentido físico.</li>
	</ul>

	<p>Con esta función, se garantiza que cada punto de la nube reconstruida tiene una base sólida desde el punto de vista geométrico y espacial. Solo se conservan aquellos puntos que están bien triangulados y situados en la región visual válida de ambas cámaras.</p>
	
    </div>

    <!-- Contenido del sexto post -->
    <div id="post_Dia6_6" class="post-container" style="display: none;">
        <h2> Proyección y visualización </h2>
	
        <p>Tras obtener la posición 3D de un punto mediante triangulación, el siguiente paso fue representarlo de forma visual dentro del entorno. Para ello, se empleó el visor 3D de Unibotics como herramienta de visualización.</p>

	<h3>Conversión de unidades</h3>
  	<p>Como las posiciones reconstruidas están en milímetros (debido a la escala de las cámaras), fue necesario dividir por 100 para convertirlas a decímetros. Esto permite una visualización adecuada y proporcional dentro del entorno virtual.</p>

	<h3>Coloración de los puntos</h3>
	<p>Cada punto 3D se visualiza no solo por su posición, sino también por el color extraído del píxel izquierdo original. Este detalle aporta riqueza visual a la nube de puntos, ya que cada uno conserva la tonalidad del objeto real que lo originó.</p>
	<ul>
	    <li>Se accede al color del píxel en la imagen izquierda (usualmente en formato BGR).</li>
	    <li>Se convierte a RGB para usarlo correctamente en el visor.</li>
	    <li>Se construye una tupla con la forma <code>(x, y, z, r, g, b)</code>.</li>
	</ul>

	<h3>Visualización en el entorno 3D</h3>
  	<p>Con la posición ajustada y el color asociado, cada punto se envía al visor con la función <code>GUI.ShowNewPoints()</code>. Así, la nube de puntos 3D se va formando de forma interactiva, punto a punto, representando fielmente la geometría y el aspecto de la escena original.</p>

  	<p>Este paso permite validar visualmente la reconstrucción y ajustar parámetros si se detectan errores en la escala, el color o la alineación de los puntos generados.</p>

    </div>

    <!-- Contenido del séptimo post -->
    <div id="post_Dia7_7" class="post-container" style="display: none;">
        <h2> Limpieza y refinamiento </h2>
        <p>Una vez generada la nube de puntos 3D, el siguiente paso fundamental fue su depuración. La reconstrucción puede incluir puntos erróneos u outliers debido a emparejamientos incorrectos, ruido o geometrías mal trianguladas.</p>

	<h3>Filtrado por distancia entre rayos</h3>
 	<p>Durante la triangulación, se calculó la distancia entre los rayos de retroproyección de cada cámara. Si esta distancia era demasiado grande, se consideraba que los rayos no se cruzaban de forma fiable, y el punto 3D correspondiente se descartaba.</p>

	<h3>Umbral sobre los parámetros de triangulación <code>t</code> y <code>s</code></h3>
	<p> Otro criterio de validación consistió en comprobar que los parámetros <code>t</code> y <code>s</code> obtenidos durante la resolución del sistema de triangulación fueran positivos y estuvieran dentro de un rango aceptable.</p>
	<p>Estos valores determinan la posición del punto a lo largo de los rayos, por lo que si son negativos o extremadamente grandes, significa que el punto reconstruido estaría fuera del campo visual de la escena. </p>
	
	<p> Gracias a la incorporación de estos dos filtros, la nube de puntos final presenta una geometría mucho más coherente y limpia,facilitando su análisis visual y reduciendo significativamente los outliers.</p>
    </div>

    <!-- Contenido del octavo post -->
    <div id="post_holonomico_8" class="post-container" style="display: none;">
        <h2> Versión Final del Controlador PID - Día 8</h2>
        <p>Tras múltiples iteraciones y pruebas, hemos llegado a la versión final del controlador PID para el coche holonómico en Unibotics. Se intentó volver a la implementación basada en la diferencia entre el centroide y el centro del frame, pero la estrategia basada en el ángulo de inclinación de la línea resultó ser la más estable.</p>
        
        <h2> Mejoras finales en el controlador PID</h2>
        <ul>
            <li><strong>Optimización de valores de PID:</strong> Se realizaron ajustes finos a <code>Kp</code>, <code>Ki</code> y <code>Kd</code> tanto en el control del volante como en la velocidad, logrando una respuesta más suave y precisa.</li>
            <li><strong>Uso de la inclinación de la línea roja:</strong> Se mantiene el cálculo del error a partir de la inclinación de la línea en diferentes franjas, lo que mejora la estabilidad en curvas.</li>
            <li><strong>Corrección de velocidad con PID:</strong> La velocidad ahora se ajusta de manera dinámica en función del ángulo de la línea, lo que evita frenadas bruscas y mejora la fluidez.</li>
            <li><strong>Estrategia de búsqueda refinada:</strong> Se mejoró la lógica de recuperación cuando el coche pierde la línea, aumentando la eficiencia en la reorientación.</li>
        </ul>

	<h2> Resultado Final: Coche Holonómico - Día 8</h2>
        <p>Este es el resultado final de nuestro coche holonómico tras todas las iteraciones y ajustes del controlador PID.</p>
        <p>Después de varias pruebas y ajustes, logramos optimizar la respuesta del coche, asegurando una estabilidad adecuada en el seguimiento de la línea roja. A continuación, puedes ver el resultado final en acción:</p>
        
        <iframe width="560" height="315" src="https://www.youtube.com/embed/ag51oN5cGHg" frameborder="0" allowfullscreen></iframe>

	<p>Se logró la estabilidad del control del coche para una velocidad máxima del móvil de 15 m/s y mínima de 7 m/s, alcanzando la vuelta completa en 73 segundos. 
	
        <h2> Observaciones Finales</h2>
        <p>Con estos ajustes, el coche ahora sigue la línea roja con alta precisión y estabilidad. La estrategia basada en el ángulo ha demostrado ser más efectiva que la basada en la posición del centroide, permitiendo mejor anticipación en curvas y una conducción más fluida.</p>
        
        <h2> Conclusión y Próximos Pasos</h2>
        <ul>
            <li> Implementar una versión mejorada del control para coches Ackerman.</li>
            <li> Explorar el uso de redes neuronales o algoritmos de aprendizaje automático para mejorar la predicción del camino.</li>
            <li> Integrar detección de obstáculos y ajustes en tiempo real.</li>
        </ul>
        
        <p>💬 ¡Este ha sido un gran avance en el control del coche holonómico! ¿Qué opinas de los resultados? 🚀</p>
    </div>

    <!-- Contenido del primer post sobre el coche Ackerman -->
    <div id="post_ackerman_1" class="post-container" style="display: none;">
        <h2> Introducción al Coche Ackerman - Primera Iteración</h2>
        <p>Después de haber implementado exitosamente el controlador PID para el coche holonómico, hemos decidido adaptar este sistema para el coche tipo Ackerman. Esto nos permitirá evaluar cómo el control PID se comporta en un sistema con restricciones en la dirección.</p>
        
        <h2> Principales diferencias con el coche holonómico</h2>
        <ul>
            <li><strong>Sistema de dirección restringida:</strong> A diferencia del holonómico, el coche Ackerman solo puede girar cambiando el ángulo de las ruedas delanteras.</li>
            <li><strong>Adaptación del PID:</strong> Se ajustaron las constantes del PID para manejar mejor la dinámica del Ackerman.</li>
            <li><strong>Manejo de velocidad optimizado:</strong> La velocidad ahora se adapta en función de la curvatura detectada para evitar derrapes.</li>
        </ul>
        
        <h2> Implementación inicial</h2>
        <p>En esta primera iteración, hemos mantenido la estrategia de detección de la línea roja usando OpenCV y segmentación en HSV. También implementamos el cálculo de la inclinación de la línea para ajustar la velocidad en tiempo real.</p>
        
        <h2> Próximos pasos</h2>
        <ul>
            <li> Refinar los valores del PID para mejorar la respuesta del coche en curvas cerradas.</li>
            <li> Implementar una estrategia de recuperación en caso de que el coche pierda la línea.</li>
            <li> Ajustar el modelo para mejorar la estabilidad en rectas y transiciones suaves en curvas.</li>
        </ul>      
    </div>

    <!-- Contenido del segundo post sobre el coche Ackerman -->
    <div id="post_ackerman_2" class="post-container" style="display: none;">
        <h2> Simplificación del Cálculo del Error - Segunda Iteración</h2>
        <p>En esta nueva versión del controlador PID para el coche Ackerman, hemos realizado un cambio significativo en la forma en la que calculamos el error para la corrección de dirección y velocidad.</p>
        
        <h2> Principales Cambios</h2>
        <ul>
            <li><strong>Eliminación del cálculo basado en la inclinación de la línea:</strong> Ahora solo se mide la diferencia del centro de la línea con el centro del frame.</li>
            <li><strong>Simplificación del código:</strong> Se redujo la cantidad de cálculos, lo que mejora la eficiencia y estabilidad del sistema.</li>
            <li><strong>PID más controlado:</strong> Ajustamos los valores del PID para reducir oscilaciones innecesarias.</li>
        </ul>
        
        <h2> Efectos de este Cambio</h2>
        <p>Gracias a esta simplificación, el código ahora es más ligero y eficiente. Sin embargo, esto también significa que el coche podría ser menos preciso en curvas cerradas, ya que no anticipa cambios de dirección mediante la inclinación de la línea roja.</p>
        
        <h2> Próximos Pasos</h2>
        <ul>
            <li> Evaluar si la simplificación afecta el rendimiento en circuitos con curvas cerradas.</li>
            <li> Ajustar los valores del PID para optimizar el seguimiento de la línea.</li>
            <li> Explorar la posibilidad de combinar la nueva estrategia con un modelo predictivo.</li>
        </ul>
    </div>

    <!-- Contenido del tercer post sobre el coche Ackerman -->
    <div id="post_ackerman_3" class="post-container" style="display: none;">
        <h2> Introducción de la Banda Muerta - Tercera Iteración</h2>
        <p>En esta versión del controlador PID para el coche Ackerman, hemos introducido la banda muerta en la corrección de la dirección, lo que permite evitar pequeñas oscilaciones innecesarias en la dirección.</p>
        
        <h2> Principales Mejoras</h2>
        <ul>
            <li><strong>Implementación de una banda muerta en el control del volante:</strong> Se evita que el coche haga correcciones innecesarias cuando el error es muy pequeño.</li>
            <li><strong>Ajuste de los valores del PID:</strong> Se incrementó Kp y Kd para mejorar la estabilidad en curvas.</li>
            <li><strong>Optimización de la velocidad:</strong> Se ha ajustado el PID de velocidad para permitir una mayor velocidad en rectas.</li>
        </ul>
        
        <h2> Impacto del Cambio</h2>
        <p>Gracias a la introducción de la banda muerta, el coche ahora es más estable en rectas y no realiza ajustes de dirección innecesarios. Esto ha permitido un incremento en la velocidad máxima sin comprometer la estabilidad en curvas.</p>
        
        <h2> Próximos Pasos</h2>
        <ul>
            <li> Evaluar el rendimiento en circuitos más complejos con diferentes tipos de curvas.</li>
            <li> Ajustar más la banda muerta para lograr un equilibrio óptimo entre estabilidad y respuesta.</li>
            <li> Explorar el uso de estrategias predictivas para anticipar mejor las curvas.</li>
        </ul>
        
    </div>

    <div id="post_ackerman_4" class="post-container" style="display: none;">
        <h2> Versión Final - Cuarta Iteración</h2>
        <p>Esta es la versión final del controlador PID para el coche Ackerman. Se han realizado los últimos ajustes para maximizar la estabilidad y eficiencia en el seguimiento de la línea roja.</p>
        
        <h2> Mejoras Finales</h2>
        <ul>
            <li><strong>Optimización de Kp y Kd:</strong> Se han ajustado para mejorar la estabilidad y minimizar oscilaciones.</li>
            <li><strong>Aceleración progresiva:</strong> Implementación de un factor de aceleración controlado para evitar cambios bruscos de velocidad.</li>
            <li><strong>Corrección de dirección limitada:</strong> Se estableció un límite en el ángulo de giro para evitar sobrecorrecciones.</li>
        </ul>
        
        <h2> Resultado Final</h2>
        <p>Con esta configuración, el coche logra un seguimiento de línea estable y eficiente. Se han minimizado los errores de oscilación y optimizado la adaptación a curvas.</p>
        
        <h2>🎥 Demostración en Video</h2>
        <p>A continuación, puedes ver el rendimiento final del coche Ackerman en la pista:</p>
        <iframe width="560" height="315" src="https://www.youtube.com/embed/3Qe5phop2FU" frameborder="0" allowfullscreen></iframe>
        
        <h2> Próximos Desafíos</h2>
        <ul>
            <li> Explorar métodos de predicción de trayectorias para mejorar la anticipación en curvas.</li>
            <li> Integrar detección de obstáculos y estrategias de evasión.</li>
            <li> Implementar modelos de aprendizaje automático para un control más adaptativo.</li>
        </ul>
        
        <p>💬 ¿Qué opinas de los resultados? ¡Déjanos tus comentarios! 🚀</p>
    </div>

    <script>
    function mostrarPost(categoria, num) {
        let categorias = ["holonomico", "ackerman"];
        
        // Ocultar todos los posts de ambas categorías
        categorias.forEach(cat => {
            for (let i = 1; i <= 9; i++) {
                let post = document.getElementById(`post_${cat}_${i}`);
                if (post) {
                    post.style.opacity = 0;
                    post.style.display = 'none';
                }
            }
        });

        // Mostrar el post seleccionado
        setTimeout(() => {
            let post = document.getElementById(`post_${categoria}_${num}`);
            if (post) {
                post.style.display = 'block';
                setTimeout(() => {
                    post.style.opacity = 1;
                    post.style.transform = 'translateY(0)';
                }, 50);
            }
        }, 200);
    }
</script>

    <!-- 📌 Footer al final de la página -->
    <footer>
        <p><a href="../index.html">Volver a la página principal</a></p>
    </footer>
</body>
</html>
